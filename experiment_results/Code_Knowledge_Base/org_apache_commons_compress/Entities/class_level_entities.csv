FEN:ID,:LABEL,Comment,Modifiers,Extends,Implements
org.apache.commons.compress.parallel.ScatterGatherBackingStoreSupplier,Interface,* Supplies {@link ScatterGatherBackingStore} instances.\n *\n * @since 1.10,public ,,
org.apache.commons.compress.parallel.FileBasedScatterGatherBackingStore,Class,* ScatterGatherBackingStore that is backed by a path.\n *\n * @since 1.10,public ,,org.apache.commons.compress.parallel.ScatterGatherBackingStore
org.apache.commons.compress.parallel.ScatterGatherBackingStore,Interface,"* <p>\n * Store intermediate payload in a scatter-gather scenario. Multiple threads write their payload to a backing store, which can subsequently be reversed to an\n * {@link InputStream} to be used as input in the gather phase.\n * </p>\n *\n * <p>\n * It is the responsibility of the allocator of an instance of this class to close this. Closing it should clear off any allocated structures and preferably\n * delete files.\n * </p>\n *\n * @since 1.10",public ,java.io.Closeable,
org.apache.commons.compress.parallel.InputStreamSupplier,Interface,"* Supplies input streams.\n *\n * Implementations are required to support thread-handover. While an instance will not be accessed concurrently by multiple threads, it will be called by a\n * different thread than it was created on.\n *\n * @since 1.10",public ,,
org.apache.commons.compress.changes.Change,Class,* Change holds meta information about a change.\n *\n * @param <E> The ArchiveEntry type.\n * @Immutable,final ,,
org.apache.commons.compress.changes.Change$ChangeType,Enum,* Enumerates types of changes.,,enum has no extends,
org.apache.commons.compress.changes.ChangeSet,Class,* ChangeSet collects and performs changes to an archive. Putting delete changes in this ChangeSet from multiple threads can cause conflicts.\n *\n * @param <E> The ArchiveEntry type.\n * @NotThreadSafe,"public , final ",,
org.apache.commons.compress.changes.ChangeSetPerformer,Class,"* Performs ChangeSet operations on a stream. This class is thread safe and can be used multiple times. It operates on a copy of the ChangeSet. If the ChangeSet\n * changes, a new Performer must be created.\n *\n * @param <I> The {@link ArchiveInputStream} type.\n * @param <O> The {@link ArchiveOutputStream} type.\n * @param <E> The {@link ArchiveEntry} type, must be compatible between the input {@code I} and output {@code O} stream types.\n * @ThreadSafe\n * @Immutable",public ,,
org.apache.commons.compress.changes.ChangeSetPerformer$ArchiveEntryIterator,Interface,* Abstracts getting entries and streams for archive entries.\n     *\n     * <p>\n     * Iterator#hasNext is not allowed to throw exceptions that's why we can't use Iterator&lt;ArchiveEntry&gt; directly - otherwise we'd need to convert\n     * exceptions thrown in ArchiveInputStream#getNextEntry.\n     * </p>,private ,,
org.apache.commons.compress.changes.ChangeSetPerformer$ArchiveInputStreamIterator,Class,No Comment,"private , static , final ",,org.apache.commons.compress.changes.ChangeSetPerformer.ArchiveEntryIterator
org.apache.commons.compress.changes.ChangeSetPerformer$ZipFileIterator,Class,No Comment,"private , static , final ",,org.apache.commons.compress.changes.ChangeSetPerformer.ArchiveEntryIterator
org.apache.commons.compress.changes.ChangeSetResults,Class,* Stores the results of a performed {@link ChangeSet} operation.,public ,,
org.apache.commons.compress.harmony.unpack200.IcTuple,Class,"* An IcTuple is the set of information that describes an inner class.\n *\n * C is the fully qualified class name<br>\n * F is the flags<br>\n * C2 is the outer class name, or null if it can be inferred from C<br>\n * N is the inner class name, or null if it can be inferred from C<br>",public ,,
org.apache.commons.compress.harmony.unpack200.BcBands,Class,* Bytecode bands.,public ,org.apache.commons.compress.harmony.unpack200.BandSet,
org.apache.commons.compress.harmony.unpack200.SegmentOptions,Class,"* Stores the combinations of bit flags that can be used in the segment header options. Whilst this could be defined in {@link Segment}, it's cleaner to pull it\n * out into a separate class, not least because methods can then be used to determine the semantic meaning of the flags. In languages with a pre-processor,\n * these may be defined by macros that do bitflag manipulation instead.",public ,,
org.apache.commons.compress.harmony.unpack200.SegmentConstantPoolArrayCache,Class,* The SegmentConstantPool spends a lot of time searching through large arrays of Strings looking for matches. This can be sped up by caching the arrays in\n * HashMaps so the String keys are looked up and resolve to positions in the array rather than iterating through the arrays each time.\n *\n * Because the arrays only grow (never shrink or change) we can use the last known size as a way to determine if the array has changed.\n *\n * Note that this cache must be synchronized externally if it is shared.,public ,,
org.apache.commons.compress.harmony.unpack200.SegmentConstantPoolArrayCache$CachedArray,Class,* Keeps track of the last known size of an array as well as a HashMap that knows the mapping from element values to the indices of the array\n     * which contain that value.,protected ,,
org.apache.commons.compress.harmony.unpack200.MetadataBandGroup,Class,"* A group of metadata bands, such as class_RVA_bands, method_AD_bands etc.",public ,,
org.apache.commons.compress.harmony.unpack200.Segment,Class,"* A Pack200 archive consists of one or more segments. Each segment is stand-alone, in the sense that every segment has the magic number header; thus, every\n * segment is also a valid archive. However, it is possible to combine (non-GZipped) archives into a single large archive by concatenation alone. Thus, all the\n * hard work in unpacking an archive falls to understanding a segment.\n * <p>\n * The first component of a segment is the header; this contains (amongst other things) the expected counts of constant pool entries, which in turn defines how\n * many values need to be read from the stream. Because values are variable width (see {@link Codec}), it is not possible to calculate the start of the next\n * segment, although one of the header values does hint at the size of the segment if non-zero, which can be used for buffering purposes.\n * </p>\n * <p>\n * Note that this does not perform any buffering of the input stream; each value will be read on a byte-by-byte basis. It does not perform GZip decompression\n * automatically; both of these are expected to be done by the caller if the stream has the magic header for GZip streams ({@link GZIPInputStream#GZIP_MAGIC}).\n * In any case, if GZip decompression is being performed the input stream will be buffered at a higher level, and thus this can read on a byte-oriented basis.\n * </p>\n * <p>\n * Format:\n * </p>\n * <pre>\n *   pack200_archive:\n *      (pack200_segment)+\n *\n *   pack200_segment:\n *      segment_header\n *      *band_headers :BYTE1\n *      cp_bands\n *      attr_definition_bands\n *      ic_bands\n *      class_bands\n *      bc_bands\n *      file_bands\n * </pre>",public ,,
org.apache.commons.compress.harmony.unpack200.Pack200UnpackerAdapter,Class,* This class provides the binding between the standard Pack200 interface and the internal interface for (un)packing.,public ,org.apache.commons.compress.harmony.pack200.Pack200Adapter,org.apache.commons.compress.java.util.jar.Pack200.Unpacker
org.apache.commons.compress.harmony.unpack200.AttributeLayoutMap,Class,"* Stores a mapping from attribute names to their corresponding layout types. Note that names of attribute layouts and their formats are <em>not</em>\n * internationalized, and should not be translated.",public ,,
org.apache.commons.compress.harmony.unpack200.SegmentHeader,Class,* SegmentHeader is the header band of a {@link Segment},public ,,
org.apache.commons.compress.harmony.unpack200.SegmentUtils,Class,* Utility class for unpack200,"public , final ",,
org.apache.commons.compress.harmony.unpack200.AttributeLayout,Class,* Defines a layout that describes how an attribute will be transmitted.,public ,,org.apache.commons.compress.harmony.unpack200.IMatcher
org.apache.commons.compress.harmony.unpack200.AttrDefinitionBands,Class,* Attribute definition bands are the set of bands used to define extra attributes transmitted in the archive.,public ,org.apache.commons.compress.harmony.unpack200.BandSet,
org.apache.commons.compress.harmony.unpack200.CpBands,Class,* Constant Pool bands,public ,org.apache.commons.compress.harmony.unpack200.BandSet,
org.apache.commons.compress.harmony.unpack200.NewAttributeBands,Class,* Sets of bands relating to a non-predefined attribute,public ,org.apache.commons.compress.harmony.unpack200.BandSet,
org.apache.commons.compress.harmony.unpack200.NewAttributeBands$AttributeLayoutElement,Interface,"* An AttributeLayoutElement is a part of an attribute layout and has one or more bands associated with it, which transmit the AttributeElement data for\n     * successive Attributes of this type.",private ,,
org.apache.commons.compress.harmony.unpack200.NewAttributeBands$Call,Class,No Comment,public ,org.apache.commons.compress.harmony.unpack200.NewAttributeBands.LayoutElement,
org.apache.commons.compress.harmony.unpack200.NewAttributeBands$Callable,Class,No Comment,"public , static ",,org.apache.commons.compress.harmony.unpack200.NewAttributeBands.AttributeLayoutElement
org.apache.commons.compress.harmony.unpack200.NewAttributeBands$Integral,Class,No Comment,public ,org.apache.commons.compress.harmony.unpack200.NewAttributeBands.LayoutElement,
org.apache.commons.compress.harmony.unpack200.NewAttributeBands$Reference,Class,* Constant Pool Reference,public ,org.apache.commons.compress.harmony.unpack200.NewAttributeBands.LayoutElement,
org.apache.commons.compress.harmony.unpack200.NewAttributeBands$Replication,Class,"* A replication is an array of layout elements, with an associated count",public ,org.apache.commons.compress.harmony.unpack200.NewAttributeBands.LayoutElement,
org.apache.commons.compress.harmony.unpack200.NewAttributeBands$Union,Class,* A Union is a type of layout element where the tag value acts as a selector for one of the union cases,public ,org.apache.commons.compress.harmony.unpack200.NewAttributeBands.LayoutElement,
org.apache.commons.compress.harmony.unpack200.NewAttributeBands$UnionCase,Class,* A Union case,public ,org.apache.commons.compress.harmony.unpack200.NewAttributeBands.LayoutElement,
org.apache.commons.compress.harmony.unpack200.NewAttributeBands$LayoutElement,Abstract Class,No Comment,"private , abstract , static ",,org.apache.commons.compress.harmony.unpack200.NewAttributeBands.AttributeLayoutElement
org.apache.commons.compress.harmony.unpack200.SegmentConstantPool,Class,* Manages the constant pool used for re-creating class files.,public ,,
org.apache.commons.compress.harmony.unpack200.bytecode.CPClass,Class,* Constant pool entry for a class,public ,org.apache.commons.compress.harmony.unpack200.bytecode.ConstantPoolEntry,
org.apache.commons.compress.harmony.unpack200.bytecode.CPRef,Abstract Class,"* Abstract superclass for reference constant pool entries, such as a method or field reference.","public , abstract ",org.apache.commons.compress.harmony.unpack200.bytecode.ConstantPoolEntry,
org.apache.commons.compress.harmony.unpack200.bytecode.CPMember,Class,"* Superclass for member constant pool entries, such as fields or methods.",public ,org.apache.commons.compress.harmony.unpack200.bytecode.ClassFileEntry,
org.apache.commons.compress.harmony.unpack200.bytecode.ExceptionTableEntry,Class,* An entry in an exception table.,public ,,
org.apache.commons.compress.harmony.unpack200.bytecode.CPFloat,Class,* Float constant pool entry.,public ,org.apache.commons.compress.harmony.unpack200.bytecode.CPConstantNumber,
org.apache.commons.compress.harmony.unpack200.bytecode.CPDouble,Class,* Double constant pool entry.,public ,org.apache.commons.compress.harmony.unpack200.bytecode.CPConstantNumber,
org.apache.commons.compress.harmony.unpack200.bytecode.ConstantPoolEntry,Abstract Class,* Abstract superclass for constant pool entries,"public , abstract ",org.apache.commons.compress.harmony.unpack200.bytecode.ClassFileEntry,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.FieldRefForm,Class,* This class implements the byte code form for those bytecodes which have field references (and only field references).,public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.ReferenceForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.LocalForm,Class,"* This class implements the byte code form for those bytecodes which have float references (and only float references). This excludes iinc (which has its own\n * form, IincForm).",public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.ByteCodeForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.VariableInstructionForm,Abstract Class,* This abstract class implements the common code for instructions which have variable lengths. This is currently the *switch instructions and some wide (_w)\n * instructions.,"public , abstract ",org.apache.commons.compress.harmony.unpack200.bytecode.forms.ByteCodeForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.SwitchForm,Abstract Class,No Comment,"public , abstract ",org.apache.commons.compress.harmony.unpack200.bytecode.forms.VariableInstructionForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.FloatRefForm,Class,* This class implements the byte code form for those bytecodes which have float references (and only float references).,public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.SingleByteReferenceForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.SingleByteReferenceForm,Abstract Class,"* Some bytecodes (such as (a)ldc, fldc and ildc) have single-byte references to the class pool. This class is the abstract superclass of those classes.","public , abstract ",org.apache.commons.compress.harmony.unpack200.bytecode.forms.ReferenceForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.InitMethodReferenceForm,Abstract Class,* Abstract superclass of those classes which look up init methods (these are class specific methods). They use getInitMethodPoolEntry to find the methods.,"public , abstract ",org.apache.commons.compress.harmony.unpack200.bytecode.forms.ClassSpecificReferenceForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.ByteForm,Class,* This class implements the form for bytecodes which have single byte operands.,public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.ByteCodeForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.ShortForm,Class,* This class implements the form for bytecodes which have short operands only.,public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.ByteCodeForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.ThisFieldRefForm,Class,"* This class implements references to fields defined in the current class, which is set by this class in the OperandManager. Pack200 allows the current class\n * to be inferred from context; this class tracks previous field reference classes to allow this.",public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.ClassSpecificReferenceForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.ClassRefForm,Class,* This class implements the byte code form for those bytecodes which have class references (and only class references).,public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.ReferenceForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.NewInitMethodRefForm,Class,"* This class is used to determine which init method should be called, based on the last class which was sent a constructor message.",public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.InitMethodReferenceForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.ThisInitMethodRefForm,Class,"* This class is used to determine which init method should be called, based on the last current class reference.",public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.InitMethodReferenceForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.WideForm,Class,"* This class implements the byte code form for the wide instruction. Unlike other instructions, it can take multiple forms, depending on what is being widened.",public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.VariableInstructionForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.IntRefForm,Class,* This class implements the byte code form for those bytecodes which have int references (and only int references).,public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.SingleByteReferenceForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.NoArgumentForm,Class,* This class implements the byte code form of all bytecodes which either have no operands (such as nop) or have all their operands passed on the stack (not\n * encoded as bytes in the bytecode streams).,public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.ByteCodeForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.NewClassRefForm,Class,* This class is an extension of the ClassRefForm. It has two purposes: 1. To keep track of the last type used in a new() instruction in the current class. 2.\n * To allow the sender to create instances of either a specified class (which then becomes the new class) or the last used new class.,public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.ClassRefForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.ThisMethodRefForm,Class,"* This class implements references to methods defined in the current class, which is set by this class in the OperandManager. Pack200 allows the current class\n * to be inferred from context; this class tracks previous method reference current classes to allow this.",public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.ClassSpecificReferenceForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.ByteCodeForm,Abstract Class,* Abstract byte code form.,"public , abstract ",,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.NarrowClassRefForm,Class,"* This class is used for representations of cldc and cldc_w. In these cases, a narrow class ref has one byte and a wide class ref has two bytes.",public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.ClassRefForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.DoubleForm,Class,* This class implements the byte code form for those bytecodes which have double references (and only double references).,public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.ReferenceForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.MultiANewArrayForm,Class,* This class implements the byte code form for the multianewarray instruction. It has a class reference and a byte operand.\n *\n * MultiANewArrayForms (like other anewarray forms) do not track the last new().,public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.ClassRefForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.IMethodRefForm,Class,* This class implements the byte code form for those bytecodes which have IMethod references (and only IMethod references).,public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.ReferenceForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.MethodRefForm,Class,* This class implements the byte code form for those bytecodes which have regular method references (and only regular method references). These are:\n * invokevirtual invokespecial invokestatic Class-specific references to methods are subclasses of ClassSpecificReferenceForm instead.,public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.ReferenceForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.SuperFieldRefForm,Class,"* This class implements references to fields defined in the superclass, which is set by this class in the OperandManager. Pack200 allows the superclass to be\n * inferred from context; this class tracks previous field reference superclasses to allow this.",public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.ClassSpecificReferenceForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.ClassSpecificReferenceForm,Abstract Class,"* Abstract superclass of all classes that have class-specific references to constant pool information. These classes have a context (a string representing a\n * pack200 class) i.e., they send getClassSpecificPoolEntry instead of getConstantPoolEntry.","public , abstract ",org.apache.commons.compress.harmony.unpack200.bytecode.forms.ReferenceForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.ReferenceForm,Abstract Class,* Abstract class of all ByteCodeForms which add a nested entry from the globalConstantPool.,"public , abstract ",org.apache.commons.compress.harmony.unpack200.bytecode.forms.ByteCodeForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.TableSwitchForm,Class,No Comment,public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.SwitchForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.LongForm,Class,* This class implements the byte code form for those bytecodes which have long references (and only long references).,public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.ReferenceForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.SuperMethodRefForm,Class,"* This class implements references to methods defined in the superclass, which is set by this class in the OperandManager. Pack200 allows the superclass to be\n * inferred from context; this class tracks previous method reference superclasses to allow this.",public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.ClassSpecificReferenceForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.IincForm,Class,* This class implements the byte code form for the iinc instruction. It has a local reference and a byte operand.,public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.ByteCodeForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.LookupSwitchForm,Class,No Comment,public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.SwitchForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.SuperInitMethodRefForm,Class,"* This class is used to determine which init method should be called, based on the last superclass reference.",public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.InitMethodReferenceForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.StringRefForm,Class,* This class implements the byte code form for those bytecodes which have string references (and only string references).,public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.SingleByteReferenceForm,
org.apache.commons.compress.harmony.unpack200.bytecode.forms.LabelForm,Class,* This class implements the byte code form for those bytecodes which have label references (and only label references).,public ,org.apache.commons.compress.harmony.unpack200.bytecode.forms.ByteCodeForm,
org.apache.commons.compress.harmony.unpack200.bytecode.EnclosingMethodAttribute,Class,* Enclosing method class file attribute.,public ,org.apache.commons.compress.harmony.unpack200.bytecode.Attribute,
org.apache.commons.compress.harmony.unpack200.bytecode.LocalVariableTypeTableAttribute,Class,* Local variable type table.,public ,org.apache.commons.compress.harmony.unpack200.bytecode.BCIRenumberedAttribute,
org.apache.commons.compress.harmony.unpack200.bytecode.ClassConstantPool,Class,* The Class constant pool,public ,,
org.apache.commons.compress.harmony.unpack200.bytecode.RuntimeVisibleorInvisibleAnnotationsAttribute,Class,"* Annotations class file attribute, either a RuntimeVisibleAnnotations attribute or a RuntimeInvisibleAnnotations attribute.",public ,org.apache.commons.compress.harmony.unpack200.bytecode.AnnotationsAttribute,
org.apache.commons.compress.harmony.unpack200.bytecode.ClassFile,Class,* ClassFile is used to represent and write out Java class files.,public ,,
org.apache.commons.compress.harmony.unpack200.bytecode.BCIRenumberedAttribute,Abstract Class,* Abstract superclass for attributes that have some part encoded with a BCI renumbering,"public , abstract ",org.apache.commons.compress.harmony.unpack200.bytecode.Attribute,
org.apache.commons.compress.harmony.unpack200.bytecode.DeprecatedAttribute,Class,* Deprecated class file attribute.,public ,org.apache.commons.compress.harmony.unpack200.bytecode.Attribute,
org.apache.commons.compress.harmony.unpack200.bytecode.CPLong,Class,* Long constant pool entry.,public ,org.apache.commons.compress.harmony.unpack200.bytecode.CPConstantNumber,
org.apache.commons.compress.harmony.unpack200.bytecode.LocalVariableTableAttribute,Class,* Local variable table,public ,org.apache.commons.compress.harmony.unpack200.bytecode.BCIRenumberedAttribute,
org.apache.commons.compress.harmony.unpack200.bytecode.LineNumberTableAttribute,Class,* Line number table,public ,org.apache.commons.compress.harmony.unpack200.bytecode.BCIRenumberedAttribute,
org.apache.commons.compress.harmony.unpack200.bytecode.CPUTF8,Class,"* UTF8 constant pool entry, used for storing long Strings.",public ,org.apache.commons.compress.harmony.unpack200.bytecode.ConstantPoolEntry,
org.apache.commons.compress.harmony.unpack200.bytecode.ClassFileEntry,Abstract Class,* The abstract superclass for all types of class file entries.,"public , abstract ",,
org.apache.commons.compress.harmony.unpack200.bytecode.CPConstantNumber,Abstract Class,* Abstract superclass for constant pool entries that are numbers.,"public , abstract ",org.apache.commons.compress.harmony.unpack200.bytecode.CPConstant,
org.apache.commons.compress.harmony.unpack200.bytecode.CPConstant,Abstract Class,* Abstract superclass for constant pool constant entries such as numbers or Strings,"public , abstract ",org.apache.commons.compress.harmony.unpack200.bytecode.ConstantPoolEntry,
org.apache.commons.compress.harmony.unpack200.bytecode.AnnotationDefaultAttribute,Class,* AnnotationDefault class file attribute,public ,org.apache.commons.compress.harmony.unpack200.bytecode.AnnotationsAttribute,
org.apache.commons.compress.harmony.unpack200.bytecode.CPInterfaceMethodRef,Class,* Interface method reference constant pool entry.,public ,org.apache.commons.compress.harmony.unpack200.bytecode.CPRef,
org.apache.commons.compress.harmony.unpack200.bytecode.InnerClassesAttribute,Class,* Inner classes class file attribute,public ,org.apache.commons.compress.harmony.unpack200.bytecode.Attribute,
org.apache.commons.compress.harmony.unpack200.bytecode.InnerClassesAttribute$InnerClassesEntry,Class,No Comment,"private , static , final ",,
org.apache.commons.compress.harmony.unpack200.bytecode.RuntimeVisibleorInvisibleParameterAnnotationsAttribute,Class,"* Parameter annotations class file attribute, either a RuntimeVisibleParameterAnnotations attribute or a RuntimeInvisibleParameterAnnotations attribute.",public ,org.apache.commons.compress.harmony.unpack200.bytecode.AnnotationsAttribute,
org.apache.commons.compress.harmony.unpack200.bytecode.RuntimeVisibleorInvisibleParameterAnnotationsAttribute$ParameterAnnotation,Class,* ParameterAnnotation represents the annotations on a single parameter.,"public , static ",,
org.apache.commons.compress.harmony.unpack200.bytecode.AnnotationsAttribute,Abstract Class,* Abstract superclass for Annotations attributes,"public , abstract ",org.apache.commons.compress.harmony.unpack200.bytecode.Attribute,
org.apache.commons.compress.harmony.unpack200.bytecode.AnnotationsAttribute$Annotation,Class,* Class to represent the annotation structure for class file attributes,"public , static ",,
org.apache.commons.compress.harmony.unpack200.bytecode.AnnotationsAttribute$ElementValue,Class,* Pairs a tag and value.,"public , static ",,
org.apache.commons.compress.harmony.unpack200.bytecode.CPMethod,Class,* Method constant pool entry.,public ,org.apache.commons.compress.harmony.unpack200.bytecode.CPMember,
org.apache.commons.compress.harmony.unpack200.bytecode.CPNameAndType,Class,* Name and Type pair constant pool entry.,public ,org.apache.commons.compress.harmony.unpack200.bytecode.ConstantPoolEntry,
org.apache.commons.compress.harmony.unpack200.bytecode.CPMethodRef,Class,* Method reference constant pool entry.,public ,org.apache.commons.compress.harmony.unpack200.bytecode.CPRef,
org.apache.commons.compress.harmony.unpack200.bytecode.CPInteger,Class,* Integer constant pool entry.,public ,org.apache.commons.compress.harmony.unpack200.bytecode.CPConstantNumber,
org.apache.commons.compress.harmony.unpack200.bytecode.NewAttribute,Class,* A compressor-defined class file attribute.,public ,org.apache.commons.compress.harmony.unpack200.bytecode.BCIRenumberedAttribute,
org.apache.commons.compress.harmony.unpack200.bytecode.NewAttribute$BCIndex,Class,No Comment,"private , static , final ",org.apache.commons.compress.harmony.unpack200.bytecode.NewAttribute.AbstractBcValue,
org.apache.commons.compress.harmony.unpack200.bytecode.NewAttribute$BCLength,Class,No Comment,"private , static , final ",org.apache.commons.compress.harmony.unpack200.bytecode.NewAttribute.AbstractBcValue,
org.apache.commons.compress.harmony.unpack200.bytecode.NewAttribute$BCOffset,Class,No Comment,"private , static , final ",org.apache.commons.compress.harmony.unpack200.bytecode.NewAttribute.AbstractBcValue,
org.apache.commons.compress.harmony.unpack200.bytecode.NewAttribute$AbstractBcValue,Abstract Class,Bytecode-related value (either a bytecode index or a length),"private , abstract , static ",,
org.apache.commons.compress.harmony.unpack200.bytecode.ConstantValueAttribute,Class,* An {@link Attribute} representing a constant.,public ,org.apache.commons.compress.harmony.unpack200.bytecode.Attribute,
org.apache.commons.compress.harmony.unpack200.bytecode.CPField,Class,* Field constant pool entry.,public ,org.apache.commons.compress.harmony.unpack200.bytecode.CPMember,
org.apache.commons.compress.harmony.unpack200.bytecode.CPFieldRef,Class,* Field reference constant pool entry.,public ,org.apache.commons.compress.harmony.unpack200.bytecode.ConstantPoolEntry,
org.apache.commons.compress.harmony.unpack200.bytecode.CPString,Class,* String constant pool entry.,public ,org.apache.commons.compress.harmony.unpack200.bytecode.CPConstant,
org.apache.commons.compress.harmony.unpack200.bytecode.Attribute,Abstract Class,* Abstract superclass for class file attributes,"public , abstract ",org.apache.commons.compress.harmony.unpack200.bytecode.ClassFileEntry,
org.apache.commons.compress.harmony.unpack200.bytecode.SignatureAttribute,Class,* Signature class file attribute,public ,org.apache.commons.compress.harmony.unpack200.bytecode.Attribute,
org.apache.commons.compress.harmony.unpack200.bytecode.SourceFileAttribute,Class,* Source file class file attribute,public ,org.apache.commons.compress.harmony.unpack200.bytecode.Attribute,
org.apache.commons.compress.harmony.unpack200.bytecode.OperandManager,Class,"* Tracks operands, provides methods to let other classes get next elements, and also knows about which classes have been used recently in super, this and new\n * references.",public ,,
org.apache.commons.compress.harmony.unpack200.bytecode.CodeAttribute,Class,No Comment,public ,org.apache.commons.compress.harmony.unpack200.bytecode.BCIRenumberedAttribute,
org.apache.commons.compress.harmony.unpack200.bytecode.ByteCode,Class,* A bytecode class file entry.,public ,org.apache.commons.compress.harmony.unpack200.bytecode.ClassFileEntry,
org.apache.commons.compress.harmony.unpack200.bytecode.ExceptionsAttribute,Class,* Exceptions class file attribute,public ,org.apache.commons.compress.harmony.unpack200.bytecode.Attribute,
org.apache.commons.compress.harmony.unpack200.IcBands,Class,* Inner Class Bands,public ,org.apache.commons.compress.harmony.unpack200.BandSet,
org.apache.commons.compress.harmony.unpack200.ClassBands,Class,* Class Bands,public ,org.apache.commons.compress.harmony.unpack200.BandSet,
org.apache.commons.compress.harmony.unpack200.BandSet,Abstract Class,* Abstract superclass for a set of bands.,"public , abstract ",,
org.apache.commons.compress.harmony.unpack200.FileBands,Class,"* Parses the file band headers (not including the actual bits themselves). At the end of this parse call, the input stream will be positioned at the start of\n * the file_bits themselves, and there will be Sum(file_size) bits remaining in the stream with BYTE1 compression. A decent implementation will probably just\n * stream the bytes out to the reconstituted Jar rather than caching them.",public ,org.apache.commons.compress.harmony.unpack200.BandSet,
org.apache.commons.compress.harmony.unpack200.Archive,Class,"* Archive is the main entry point to unpack200. An archive is constructed with either two file names, a pack file and an output file name or an input stream\n * and an output streams. Then {@code unpack()} is called, to unpack the pack200 archive.",public ,,
org.apache.commons.compress.harmony.unpack200.IMatcher,Interface,* Interface for a class that can perform matching on flag values.,public ,,
org.apache.commons.compress.harmony.archive.internal.nls.Messages,Class,"* This class retrieves strings from a resource bundle and returns them, formatting them with MessageFormat when required.\n * <p>\n * It is used by the system classes to provide national language support, by looking up messages in the {@code\n *    org.apache.commons.compress.harmony.archive.internal.nls.messages\n * } resource bundle. Note that if this file is not available, or an invalid key is looked up, or resource bundle support is not available, the key itself\n * will be returned as the associated message. This means that the <em>KEY</em> should a reasonable human-readable (english) string.",public ,,
org.apache.commons.compress.harmony.pack200.CPClass,Class,* Constant pool entry for a class.,public ,org.apache.commons.compress.harmony.pack200.CPConstant,
org.apache.commons.compress.harmony.pack200.BcBands,Class,* Bytecode bands (corresponds to the {@code bc_bands} set of bands in the pack200 specification),public ,org.apache.commons.compress.harmony.pack200.BandSet,
org.apache.commons.compress.harmony.pack200.CPFloat,Class,* Constant pool entry for a float.,public ,org.apache.commons.compress.harmony.pack200.CPConstant,
org.apache.commons.compress.harmony.pack200.CPDouble,Class,* Constant pool entry for a double.,public ,org.apache.commons.compress.harmony.pack200.CPConstant,
org.apache.commons.compress.harmony.pack200.ConstantPoolEntry,Abstract Class,* Abstract superclass for constant pool entries.,"public , abstract ",,
org.apache.commons.compress.harmony.pack200.CPMethodOrField,Class,* Constant pool entry for a method or field.,public ,org.apache.commons.compress.harmony.pack200.ConstantPoolEntry,java.lang.Comparable
org.apache.commons.compress.harmony.pack200.MetadataBandGroup,Class,"* A group of metadata (annotation) bands, such as class_RVA_bands, method_AD_bands etc.",public ,org.apache.commons.compress.harmony.pack200.BandSet,
org.apache.commons.compress.harmony.pack200.Segment,Class,* A Pack200 archive consists of one or more Segments.\n * <p>\n * Format:\n * </p>\n * <pre>\n *   pack200_archive:\n *      (pack200_segment)+\n *\n *   pack200_segment:\n *      segment_header\n *      *band_headers :BYTE1\n *      cp_bands\n *      attr_definition_bands\n *      ic_bands\n *      class_bands\n *      bc_bands\n *      file_bands\n * </pre>,public ,org.objectweb.asm.ClassVisitor,
org.apache.commons.compress.harmony.pack200.Segment$ArrayVisitor,Class,No Comment,public ,org.objectweb.asm.AnnotationVisitor,
org.apache.commons.compress.harmony.pack200.Segment$PassException,Class,"* Exception indicating that the class currently being visited contains an unknown attribute, which means that by default the class file needs to be passed\n     * through as-is in the file_bands rather than being packed with pack200.","public , static ",java.lang.RuntimeException,
org.apache.commons.compress.harmony.pack200.Segment$SegmentAnnotationVisitor,Class,* SegmentAnnotationVisitor implements {@code AnnotationVisitor} to visit Annotations found in a class file.,public ,org.objectweb.asm.AnnotationVisitor,
org.apache.commons.compress.harmony.pack200.Segment$SegmentFieldVisitor,Class,* SegmentFieldVisitor implements {@code FieldVisitor} to visit the metadata relating to fields in a class file.,public ,org.objectweb.asm.FieldVisitor,
org.apache.commons.compress.harmony.pack200.Segment$SegmentMethodVisitor,Class,* This class implements MethodVisitor to visit the contents and metadata related to methods in a class file.\n     *\n     * It delegates to BcBands for bytecode related visits and to ClassBands for everything else.,public ,org.objectweb.asm.MethodVisitor,
org.apache.commons.compress.harmony.pack200.CPInt,Class,* Constant pool entry for an int.,public ,org.apache.commons.compress.harmony.pack200.CPConstant,
org.apache.commons.compress.harmony.pack200.Pack200PackerAdapter,Class,"* This class provides the binding between the standard Pack200 interface and the internal interface for (un)packing. As this uses generics for the SortedMap,\n * this class must be compiled and run on a Java 1.5 system. However, Java 1.5 is not necessary to use the internal libraries for unpacking.",public ,org.apache.commons.compress.harmony.pack200.Pack200Adapter,org.apache.commons.compress.java.util.jar.Pack200.Packer
org.apache.commons.compress.harmony.pack200.Pack200Exception,Class,* Signals a problem with a Pack200 coding or decoding issue.,public ,java.io.IOException,
org.apache.commons.compress.harmony.pack200.CPLong,Class,* Constant pool entry for a long.,public ,org.apache.commons.compress.harmony.pack200.CPConstant,
org.apache.commons.compress.harmony.pack200.AttributeDefinitionBands,Class,* Attribute Definition bands define how any unknown attributes should be read by the decompressor.,public ,org.apache.commons.compress.harmony.pack200.BandSet,
org.apache.commons.compress.harmony.pack200.AttributeDefinitionBands$AttributeDefinition,Class,No Comment,"public , static ",,
org.apache.commons.compress.harmony.pack200.SegmentHeader,Class,* SegmentHeader is the header band of a {@link Segment}. Corresponds to {@code segment_header} in the pack200 specification.,public ,org.apache.commons.compress.harmony.pack200.BandSet,
org.apache.commons.compress.harmony.pack200.SegmentHeader$Counter,Class,"* Counter for major/minor class file numbers, so we can work out the default","private , static , final ",,
org.apache.commons.compress.harmony.pack200.CPSignature,Class,* Constant pool entry for a signature.,public ,org.apache.commons.compress.harmony.pack200.ConstantPoolEntry,java.lang.Comparable
org.apache.commons.compress.harmony.pack200.PackingOptions,Class,* Manages the various options available for pack200.,public ,,
org.apache.commons.compress.harmony.pack200.BHSDCodec,Class,"* A BHSD codec is a means of encoding integer values as a sequence of bytes or vice versa using a specified ""BHSD"" encoding mechanism. It uses a\n * variable-length encoding and a modified sign representation such that small numbers are represented as a single byte, whilst larger numbers take more bytes\n * to encode. The number may be signed or unsigned; if it is unsigned, it can be weighted towards positive numbers or equally distributed using a one's\n * complement. The Codec also supports delta coding, where a sequence of numbers is represented as a series of first-order differences. So a delta encoding of\n * the integers [1..10] would be represented as a sequence of 10x1s. This allows the absolute value of a coded integer to fall outside of the 'small number'\n * range, whilst still being encoded as a single byte.\n * <p>\n * A BHSD codec is configured with four parameters:\n * </p>\n * <dl>\n * <dt>B</dt>\n * <dd>The maximum number of bytes that each value is encoded as. B must be a value between [1..5]. For a pass-through coding (where each byte is encoded as\n * itself, aka {@link #BYTE1}, B is 1 (each byte takes a maximum of 1 byte).</dd>\n * <dt>H</dt>\n * <dd>The radix of the integer. Values are defined as a sequence of values, where value {@code n} is multiplied by {@code H^<sup>n</sup>}. So the number 1234\n * may be represented as the sequence 4 3 2 1 with a radix (H) of 10. Note that other permutations are also possible; 43 2 1 will also encode 1234. The\n * co-parameter L is defined as 256-H. This is important because only the last value in a sequence may be &lt; L; all prior values must be &gt; L.</dd>\n * <dt>S</dt>\n * <dd>Whether the codec represents signed values (or not). This may have 3 values; 0 (unsigned), 1 (signed, one's complement) or 2 (signed, two's\n * complement)</dd>\n * <dt>D</dt>\n * <dd>Whether the codec represents a delta encoding. This may be 0 (no delta) or 1 (delta encoding). A delta encoding of 1 indicates that values are\n * cumulative; a sequence of {@code 1 1 1 1 1} will represent the sequence {@code 1 2 3 4 5}. For this reason, the codec supports two variants of decode; one\n * {@link #decode(InputStream, long) with} and one {@link #decode(InputStream) without} a {@code last} parameter. If the codec is a non-delta encoding, then the\n * value is ignored if passed. If the codec is a delta encoding, it is a run-time error to call the value without the extra parameter, and the previous value\n * should be returned. (It was designed this way to support multi-threaded access without requiring a new instance of the Codec to be cloned for each use.)</dd>\n * </dl>\n * <p>\n * Codecs are notated as (B,H,S,D) and either D or S,D may be omitted if zero. Thus {@link #BYTE1} is denoted (1,256,0,0) or (1,256). The {@link #toString()}\n * method prints out the condensed form of the encoding. Often, the last character in the name ({@link #BYTE1}, {@link #UNSIGNED5}) gives a clue as to the B\n * value. Those that start with U ({@link #UDELTA5}, {@link #UNSIGNED5}) are unsigned; otherwise, in most cases, they are signed. The presence of the word Delta\n * ({@link #DELTA5}, {@link #UDELTA5}) indicates a delta encoding is used.\n * </p>","public , final ",org.apache.commons.compress.harmony.pack200.Codec,
org.apache.commons.compress.harmony.pack200.CPUTF8,Class,"* Constant pool entry for a UTF8 entry, used for storing long Strings.",public ,org.apache.commons.compress.harmony.pack200.ConstantPoolEntry,java.lang.Comparable
org.apache.commons.compress.harmony.pack200.CPConstant,Abstract Class,* Abstract superclass for constant pool constant entries such as numbers or Strings.\n *\n * @param <T> The CPConstant subclass.,"public , abstract ",org.apache.commons.compress.harmony.pack200.ConstantPoolEntry,java.lang.Comparable
org.apache.commons.compress.harmony.pack200.CpBands,Class,* Pack200 Constant Pool Bands,public ,org.apache.commons.compress.harmony.pack200.BandSet,
org.apache.commons.compress.harmony.pack200.NewAttributeBands,Class,"* Sets of bands relating to a non-predefined attribute that has had a layout definition given to pack200 (for example via one of the -C, -M, -F or -D command\n * line options)",public ,org.apache.commons.compress.harmony.pack200.BandSet,
org.apache.commons.compress.harmony.pack200.NewAttributeBands$AttributeLayoutElement,Interface,"* An AttributeLayoutElement is a part of an attribute layout and has one or more bands associated with it, which transmit the AttributeElement data for\n     * successive Attributes of this type.",public ,,
org.apache.commons.compress.harmony.pack200.NewAttributeBands$Call,Class,No Comment,public ,org.apache.commons.compress.harmony.pack200.NewAttributeBands.LayoutElement,
org.apache.commons.compress.harmony.pack200.NewAttributeBands$Callable,Class,No Comment,public ,,org.apache.commons.compress.harmony.pack200.NewAttributeBands.AttributeLayoutElement
org.apache.commons.compress.harmony.pack200.NewAttributeBands$Integral,Class,No Comment,public ,org.apache.commons.compress.harmony.pack200.NewAttributeBands.LayoutElement,
org.apache.commons.compress.harmony.pack200.NewAttributeBands$Reference,Class,* Constant Pool Reference,public ,org.apache.commons.compress.harmony.pack200.NewAttributeBands.LayoutElement,
org.apache.commons.compress.harmony.pack200.NewAttributeBands$Replication,Class,"* A replication is an array of layout elements, with an associated count",public ,org.apache.commons.compress.harmony.pack200.NewAttributeBands.LayoutElement,
org.apache.commons.compress.harmony.pack200.NewAttributeBands$Union,Class,* A Union is a type of layout element where the tag value acts as a selector for one of the union cases,public ,org.apache.commons.compress.harmony.pack200.NewAttributeBands.LayoutElement,
org.apache.commons.compress.harmony.pack200.NewAttributeBands$UnionCase,Class,* A Union case,public ,org.apache.commons.compress.harmony.pack200.NewAttributeBands.LayoutElement,
org.apache.commons.compress.harmony.pack200.NewAttributeBands$LayoutElement,Abstract Class,No Comment,"public , abstract ",,org.apache.commons.compress.harmony.pack200.NewAttributeBands.AttributeLayoutElement
org.apache.commons.compress.harmony.pack200.CPNameAndType,Class,* Constant pool entry for a name and type pair.,public ,org.apache.commons.compress.harmony.pack200.ConstantPoolEntry,java.lang.Comparable
org.apache.commons.compress.harmony.pack200.PackingUtils,Class,No Comment,public ,,
org.apache.commons.compress.harmony.pack200.PackingUtils$PackingLogger,Class,No Comment,"private , static , final ",java.util.logging.Logger,
org.apache.commons.compress.harmony.pack200.NewAttribute,Class,"* NewAttribute extends {@code Attribute} and manages unknown attributes encountered by ASM that have had a layout definition given to pack200 (for example via\n * one of the -C, -M, -F or -D command line options)",public ,org.objectweb.asm.Attribute,
org.apache.commons.compress.harmony.pack200.NewAttribute$ErrorAttribute,Class,"* ErrorAttribute extends {@code NewAttribute} and manages attributes encountered by ASM that have had an error action specified to pack200 (for example via\n     * one of the -C, -M, -F or -D command line options such as -Cattribute-name=error)","public , static ",org.apache.commons.compress.harmony.pack200.NewAttribute,
org.apache.commons.compress.harmony.pack200.NewAttribute$PassAttribute,Class,"* PassAttribute extends {@code NewAttribute} and manages attributes encountered by ASM that have had a pass action specified to pack200 (for example via\n     * one of the -C, -M, -F or -D command line options such as -Cattribute-name=pass)","public , static ",org.apache.commons.compress.harmony.pack200.NewAttribute,
org.apache.commons.compress.harmony.pack200.NewAttribute$StripAttribute,Class,"* StripAttribute extends {@code NewAttribute} and manages attributes encountered by ASM that have had a strip action specified to pack200 (for example via\n     * one of the -C, -M, -F or -D command line options such as -Cattribute-name=strip)","public , static ",org.apache.commons.compress.harmony.pack200.NewAttribute,
org.apache.commons.compress.harmony.pack200.Pack200Adapter,Abstract Class,* Provides generic JavaBeans support for the Pack/UnpackAdapters,"public , abstract ",,
org.apache.commons.compress.harmony.pack200.CanonicalCodecFamilies,Class,* Sets of codecs that share characteristics. Mainly used for different effort compression heuristics in BandSet.,public ,,
org.apache.commons.compress.harmony.pack200.CPString,Class,* Constant pool entry for a String.,public ,org.apache.commons.compress.harmony.pack200.CPConstant,
org.apache.commons.compress.harmony.pack200.IcBands,Class,* Inner class bands (corresponds to the {@code ic_bands} set of bands in the pack200 specification),public ,org.apache.commons.compress.harmony.pack200.BandSet,
org.apache.commons.compress.harmony.pack200.IcBands$IcTuple,Class,No Comment,"static , final ",,java.lang.Comparable
org.apache.commons.compress.harmony.pack200.IntList,Class,"* IntList is based on {@link java.util.ArrayList}, but is written specifically for ints in order to reduce boxing and unboxing to Integers, reduce the memory\n * required and improve performance of pack200.",public ,,
org.apache.commons.compress.harmony.pack200.ClassBands,Class,* Class bands (corresponds to the {@code class_bands} set of bands in the pack200 specification),public ,org.apache.commons.compress.harmony.pack200.BandSet,
org.apache.commons.compress.harmony.pack200.ClassBands$TempParamAnnotation,Class,No Comment,"private , static , final ",,
org.apache.commons.compress.harmony.pack200.BandSet,Abstract Class,* Abstract superclass for a set of bands,"public , abstract ",,
org.apache.commons.compress.harmony.pack200.BandSet$BandAnalysisResults,Class,* Results obtained by trying different Codecs to encode a band,public ,,
org.apache.commons.compress.harmony.pack200.BandSet$BandData,Class,"* BandData represents information about a band, for example largest value etc and is used in the heuristics that calculate whether an alternative Codec\n     * could make the encoded band smaller.",public ,,
org.apache.commons.compress.harmony.pack200.FileBands,Class,* Bands containing information about files in the pack200 archive and the file contents for non-class-files. Corresponds to the {@code file_bands} set of bands\n * described in the specification.,public ,org.apache.commons.compress.harmony.pack200.BandSet,
org.apache.commons.compress.harmony.pack200.RunCodec,Class,"* A run codec is a grouping of two nested codecs; K values are decoded from the first codec, and the remaining codes are decoded from the remaining codec. Note\n * that since this codec maintains state, the instances are not reusable.",public ,org.apache.commons.compress.harmony.pack200.Codec,
org.apache.commons.compress.harmony.pack200.Codec,Abstract Class,"* A Codec allows a sequence of bytes to be decoded into integer values (or vice versa).\n * <p>\n * There are a number of standard Codecs ({@link #UDELTA5}, {@link #UNSIGNED5}, {@link #BYTE1}, {@link #CHAR3}) that are used in the implementation of many\n * bands; but there are a variety of other ones, and indeed the specification assumes that other combinations of values can result in more specific and\n * efficient formats. There are also a sequence of canonical encodings defined by the Pack200 specification, which allow a Codec to be referred to by canonical\n * number. {@link CodecEncoding#getCodec(int, InputStream, Codec)})\n * </p>","public , abstract ",,
org.apache.commons.compress.harmony.pack200.PopulationCodec,Class,"* A PopulationCodec is a Codec that is well suited to encoding data that shows statistical or repetitive patterns, containing for example a few numbers which\n * are repeated a lot throughout the set, but not necessarily sequentially.",public ,org.apache.commons.compress.harmony.pack200.Codec,
org.apache.commons.compress.harmony.pack200.CodecEncoding,Class,* CodecEncoding is used to get the right Codec for a given meta-encoding.,public ,,
org.apache.commons.compress.harmony.pack200.Pack200ClassReader,Class,* Wrapper for ClassReader that enables pack200 to obtain extra class file information,public ,org.objectweb.asm.ClassReader,
org.apache.commons.compress.harmony.pack200.Archive,Class,"* Archive is the main entry point to pack200 and represents a packed archive. An archive is constructed with either a JarInputStream and an output stream or a\n * JarFile as input and an OutputStream. Options can be set, then {@code pack()} is called, to pack the Jar file into a pack200 archive.",public ,,
org.apache.commons.compress.harmony.pack200.Archive$PackingFile,Class,No Comment,"static , final ",,
org.apache.commons.compress.harmony.pack200.Archive$SegmentUnit,Class,No Comment,"static , final ",,
org.apache.commons.compress.archivers.StreamingNotSupportedException,Class,* Exception thrown by ArchiveStreamFactory if a format is requested/detected that doesn't support streaming.\n *\n * @since 1.8,public ,org.apache.commons.compress.archivers.ArchiveException,
org.apache.commons.compress.archivers.ArchiveStreamFactory,Class,"* Creates an Archive[In|Out]putStreams from names or the first bytes of the InputStream. In order to add other implementations, you should extend\n * ArchiveStreamFactory and override the appropriate methods (and call their implementation from super of course).\n *\n * Compressing a ZIP-File:\n *\n * <pre>\n * final OutputStream out = Files.newOutputStream(output.toPath());\n * ArchiveOutputStream os = new ArchiveStreamFactory().createArchiveOutputStream(ArchiveStreamFactory.ZIP, out);\n *\n * os.putArchiveEntry(new ZipArchiveEntry(""testdata/test1.xml""));\n * IOUtils.copy(Files.newInputStream(file1.toPath()), os);\n * os.closeArchiveEntry();\n *\n * os.putArchiveEntry(new ZipArchiveEntry(""testdata/test2.xml""));\n * IOUtils.copy(Files.newInputStream(file2.toPath()), os);\n * os.closeArchiveEntry();\n * os.close();\n * </pre>\n *\n * Decompressing a ZIP-File:\n *\n * <pre>\n * final InputStream is = Files.newInputStream(input.toPath());\n * ArchiveInputStream in = new ArchiveStreamFactory().createArchiveInputStream(ArchiveStreamFactory.ZIP, is);\n * ZipArchiveEntry entry = (ZipArchiveEntry) in.getNextEntry();\n * OutputStream out = Files.newOutputStream(dir.toPath().resolve(entry.getName()));\n * IOUtils.copy(in, out);\n * out.close();\n * in.close();\n * </pre>\n *\n * @Immutable provided that the deprecated method setEntryEncoding is not used.\n * @ThreadSafe even if the deprecated method setEntryEncoding is used",public ,,org.apache.commons.compress.archivers.ArchiveStreamProvider
org.apache.commons.compress.archivers.ArchiveException,Class,* Signals that an Archive exception of some sort has occurred.,public ,java.io.IOException,
org.apache.commons.compress.archivers.dump.DumpArchiveInputStream,Class,"* The DumpArchiveInputStream reads a Unix dump archive as an InputStream. Methods are provided to position at each successive entry in the archive, and the\n * read each entry as a normal input stream using read().\n * <p>\n * There doesn't seem to exist a hint on the encoding of string values in any piece documentation. Given the main purpose of dump/restore is backing up a system\n * it seems very likely the format uses the current default encoding of the system.\n * </p>\n * @NotThreadSafe\n * @since 1.3",public ,org.apache.commons.compress.archivers.ArchiveInputStream,
org.apache.commons.compress.archivers.dump.DumpArchiveException,Class,* Dump Archive Exception,public ,java.io.IOException,
org.apache.commons.compress.archivers.dump.Dirent,Class,* Directory entry.,final ,,
org.apache.commons.compress.archivers.dump.DumpArchiveSummary,Class,"* This class represents identifying information about a Dump archive volume. It consists the archive's dump date, label, hostname, device name and possibly\n * last mount point plus the volume's volume id and first record number.\n * <p>\n * For the corresponding C structure see the header of {@link DumpArchiveEntry}.\n * </p>",public ,,
org.apache.commons.compress.archivers.dump.UnsupportedCompressionAlgorithmException,Class,* Unsupported compression algorithm. The dump archive uses an unsupported compression algorithm (BZLIB2 or LZO).,public ,org.apache.commons.compress.archivers.dump.DumpArchiveException,
org.apache.commons.compress.archivers.dump.DumpArchiveUtil,Class,* Various utilities for dump archives.,final ,,
org.apache.commons.compress.archivers.dump.InvalidFormatException,Class,* Invalid Format Exception. There was an error decoding a tape segment header.,public ,org.apache.commons.compress.archivers.dump.DumpArchiveException,
org.apache.commons.compress.archivers.dump.TapeInputStream,Class,* Filter stream that mimics a physical tape drive capable of compressing the data stream.\n *\n * @NotThreadSafe,final ,java.io.FilterInputStream,
org.apache.commons.compress.archivers.dump.DumpArchiveEntry,Class,"* This class represents an entry in a Dump archive. It consists of the entry's header, the entry's File and any extended attributes.\n * <p>\n * DumpEntries that are created from the header bytes read from an archive are instantiated with the DumpArchiveEntry( byte[] ) constructor. These entries will\n * be used when extracting from or listing the contents of an archive. These entries have their header filled in using the header bytes. They also set the File\n * to null, since they reference an archive entry not a file.\n * <p>\n * DumpEntries can also be constructed from nothing but a name. This allows the programmer to construct the entry by hand, for instance when only an InputStream\n * is available for writing to the archive, and the header information is constructed from other information. In this case the header fields are set to defaults\n * and the File is set to null.\n *\n * <p>\n * The C structure for a Dump Entry's header is:\n *\n * <pre>\n * #define TP_BSIZE    1024          // size of each file block\n * #define NTREC       10            // number of blocks to write at once\n * #define HIGHDENSITYTREC 32        // number of blocks to write on high-density tapes\n * #define TP_NINDIR   (TP_BSIZE/2)  // number if indirect inodes in record\n * #define TP_NINOS    (TP_NINDIR / sizeof (int32_t))\n * #define LBLSIZE     16\n * #define NAMELEN     64\n *\n * #define OFS_MAGIC     (int) 60011  // old format magic value\n * #define NFS_MAGIC     (int) 60012  // new format magic value\n * #define FS_UFS2_MAGIC (int) 0x19540119\n * #define CHECKSUM      (int) 84446  // constant used in checksum algorithm\n *\n * struct  s_spcl {\n *   int32_t c_type;             // record type (see below)\n *   int32_t <strong>c_date</strong>;             // date of this dump\n *   int32_t <strong>c_ddate</strong>;            // date of previous dump\n *   int32_t c_volume;           // dump volume number\n *   u_int32_t c_tapea;          // logical block of this record\n *   dump_ino_t c_ino;           // number of inode\n *   int32_t <strong>c_magic</strong>;            // magic number (see above)\n *   int32_t c_checksum;         // record checksum\n * #ifdef  __linux__\n *   struct  new_bsd_inode c_dinode;\n * #else\n * #ifdef sunos\n *   struct  new_bsd_inode c_dinode;\n * #else\n *   struct  dinode  c_dinode;   // ownership and mode of inode\n * #endif\n * #endif\n *   int32_t c_count;            // number of valid c_addr entries\n *   union u_data c_data;        // see above\n *   char    <strong>c_label[LBLSIZE]</strong>;   // dump label\n *   int32_t <strong>c_level</strong>;            // level of this dump\n *   char    <strong>c_filesys[NAMELEN]</strong>; // name of dumpped file system\n *   char    <strong>c_dev[NAMELEN]</strong>;     // name of dumpped device\n *   char    <strong>c_host[NAMELEN]</strong>;    // name of dumpped host\n *   int32_t c_flags;            // additional information (see below)\n *   int32_t c_firstrec;         // first record on volume\n *   int32_t c_ntrec;            // blocksize on volume\n *   int32_t c_extattributes;    // additional inode info (see below)\n *   int32_t c_spare[30];        // reserved for future uses\n * } s_spcl;\n *\n * //\n * // flag values\n * //\n * #define DR_NEWHEADER     0x0001  // new format tape header\n * #define DR_NEWINODEFMT   0x0002  // new format inodes on tape\n * #define DR_COMPRESSED    0x0080  // dump tape is compressed\n * #define DR_METAONLY      0x0100  // only the metadata of the inode has been dumped\n * #define DR_INODEINFO     0x0002  // [SIC] TS_END header contains c_inos information\n * #define DR_EXTATTRIBUTES 0x8000\n *\n * //\n * // extattributes inode info\n * //\n * #define EXT_REGULAR         0\n * #define EXT_MACOSFNDRINFO   1\n * #define EXT_MACOSRESFORK    2\n * #define EXT_XATTR           3\n *\n * // used for EA on tape\n * #define EXT2_GOOD_OLD_INODE_SIZE    128\n * #define EXT2_XATTR_MAGIC        0xEA020000  // block EA\n * #define EXT2_XATTR_MAGIC2       0xEA020001  // in inode EA\n * </pre>\n * <p>\n * The fields in <strong>bold</strong> are the same for all blocks. (This permitted multiple dumps to be written to a single tape.)\n * </p>\n *\n * <p>\n * The C structure for the inode (file) information is:\n *\n * <pre>\n * struct bsdtimeval {           //  **** alpha-*-linux is deviant\n *   __u32   tv_sec;\n *   __u32   tv_usec;\n * };\n *\n * #define NDADDR      12\n * #define NIADDR       3\n *\n * //\n * // This is the new (4.4) BSD inode structure\n * // copied from the FreeBSD 2.0 &lt;ufs/ufs/dinode.h&gt; include file\n * //\n * struct new_bsd_inode {\n *   __u16       di_mode;           // file type, standard Unix permissions\n *   __s16       di_nlink;          // number of hard links to file.\n *   union {\n *      __u16       oldids[2];\n *      __u32       inumber;\n *   }           di_u;\n *   u_quad_t    di_size;           // file size\n *   struct bsdtimeval   di_atime;  // time file was last accessed\n *   struct bsdtimeval   di_mtime;  // time file was last modified\n *   struct bsdtimeval   di_ctime;  // time file was created\n *   __u32       di_db[NDADDR];\n *   __u32       di_ib[NIADDR];\n *   __u32       di_flags;          //\n *   __s32       di_blocks;         // number of disk blocks\n *   __s32       di_gen;            // generation number\n *   __u32       di_uid;            // user id (see /etc/passwd)\n *   __u32       di_gid;            // group id (see /etc/group)\n *   __s32       di_spare[2];       // unused\n * };\n * </pre>\n * <p>\n * It is important to note that the header DOES NOT have the name of the file. It can't since hard links mean that you may have multiple file names for a single\n * physical file. You must read the contents of the directory entries to learn the mapping(s) from file name to inode.\n * </p>\n *\n * <p>\n * The C structure that indicates if a specific block is a real block that contains data or is a sparse block that is not persisted to the disk is:\n * </p>\n *\n * <pre>\n * #define TP_BSIZE    1024\n * #define TP_NINDIR   (TP_BSIZE/2)\n *\n * union u_data {\n *   char    s_addrs[TP_NINDIR]; // 1 =&gt; data; 0 =&gt; hole in inode\n *   int32_t s_inos[TP_NINOS];   // table of first inode on each volume\n * } u_data;\n * </pre>\n *\n * @NotThreadSafe",public ,,org.apache.commons.compress.archivers.ArchiveEntry
org.apache.commons.compress.archivers.dump.DumpArchiveEntry$PERMISSION,Enum,* Enumerates permissions with values.,public ,enum has no extends,
org.apache.commons.compress.archivers.dump.DumpArchiveEntry$TapeSegmentHeader,Class,* Archive entry as stored on tape. There is one TSH for (at most) every 512k in the file.,"static , final ",,
org.apache.commons.compress.archivers.dump.DumpArchiveEntry$TYPE,Enum,* Enumerates types.,public ,enum has no extends,
org.apache.commons.compress.archivers.dump.ShortFileException,Class,* Short File Exception. There was an unexpected EOF when reading the input stream.,public ,org.apache.commons.compress.archivers.dump.DumpArchiveException,
org.apache.commons.compress.archivers.dump.DumpArchiveConstants,Class,* Various constants associated with dump archives.,"public , final ",,
org.apache.commons.compress.archivers.dump.DumpArchiveConstants$COMPRESSION_TYPE,Enum,* Enumerates compression types.,public ,enum has no extends,
org.apache.commons.compress.archivers.dump.DumpArchiveConstants$SEGMENT_TYPE,Enum,* Enumerates the types of tape segment.,public ,enum has no extends,
org.apache.commons.compress.archivers.dump.UnrecognizedFormatException,Class,* Unrecognized Format Exception. This is either not a recognized dump archive or there's a bad tape segment header.,public ,org.apache.commons.compress.archivers.dump.DumpArchiveException,
org.apache.commons.compress.archivers.ArchiveStreamProvider,Interface,* Creates Archive {@link ArchiveInputStream}s and {@link ArchiveOutputStream}s.\n *\n * @since 1.13,public ,,
org.apache.commons.compress.archivers.jar.JarArchiveOutputStream,Class,* Subclass that adds a special extra field to the very first entry which allows the created archive to be used as an executable jar on Solaris.\n *\n * @NotThreadSafe,public ,org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream,
org.apache.commons.compress.archivers.jar.JarArchiveEntry,Class,* JAR archive entry.\n *\n * @NotThreadSafe (parent is not thread-safe),public ,org.apache.commons.compress.archivers.zip.ZipArchiveEntry,
org.apache.commons.compress.archivers.jar.JarArchiveInputStream,Class,* Implements an input stream that can read entries from jar files.\n *\n * @NotThreadSafe,public ,org.apache.commons.compress.archivers.zip.ZipArchiveInputStream,
org.apache.commons.compress.archivers.sevenz.StreamMap,Class,"* Map between folders, files and streams.",final ,,
org.apache.commons.compress.archivers.sevenz.StartHeader,Class,No Comment,final ,,
org.apache.commons.compress.archivers.sevenz.SevenZArchiveEntry,Class,* An entry in a 7z archive.\n *\n * @NotThreadSafe\n * @since 1.6,public ,,org.apache.commons.compress.archivers.ArchiveEntry
org.apache.commons.compress.archivers.sevenz.SevenZOutputFile,Class,* Writes a 7z file.\n *\n * @since 1.6,public ,,java.io.Closeable
org.apache.commons.compress.archivers.sevenz.SevenZOutputFile$OutputStreamWrapper,Class,No Comment,"private , final ",java.io.OutputStream,
org.apache.commons.compress.archivers.sevenz.SevenZFileOptions,Class,* Collects options for reading 7z archives.\n *\n * @since 1.19\n * @Immutable\n * @deprecated Use {@link SevenZFile.Builder}.,public ,,
org.apache.commons.compress.archivers.sevenz.SevenZFileOptions$Builder,Class,* Mutable builder for the immutable {@link SevenZFileOptions}.\n     *\n     * @since 1.19,"public , static ",,
org.apache.commons.compress.archivers.sevenz.Coder,Class,No Comment,final ,,
org.apache.commons.compress.archivers.sevenz.NID,Class,No Comment,final ,,
org.apache.commons.compress.archivers.sevenz.AES256SHA256Decoder,Class,No Comment,final ,org.apache.commons.compress.archivers.sevenz.AbstractCoder,
org.apache.commons.compress.archivers.sevenz.AES256SHA256Decoder$AES256SHA256DecoderInputStream,Class,No Comment,"private , static , final ",java.io.InputStream,
org.apache.commons.compress.archivers.sevenz.AES256SHA256Decoder$AES256SHA256DecoderOutputStream,Class,No Comment,"private , static , final ",java.io.OutputStream,
org.apache.commons.compress.archivers.sevenz.Coders,Class,No Comment,final ,,
org.apache.commons.compress.archivers.sevenz.Coders$BCJDecoder,Class,No Comment,"static , final ",org.apache.commons.compress.archivers.sevenz.AbstractCoder,
org.apache.commons.compress.archivers.sevenz.Coders$BZIP2Decoder,Class,No Comment,"static , final ",org.apache.commons.compress.archivers.sevenz.AbstractCoder,
org.apache.commons.compress.archivers.sevenz.Coders$CopyDecoder,Class,No Comment,"static , final ",org.apache.commons.compress.archivers.sevenz.AbstractCoder,
org.apache.commons.compress.archivers.sevenz.Coders$Deflate64Decoder,Class,No Comment,"static , final ",org.apache.commons.compress.archivers.sevenz.AbstractCoder,
org.apache.commons.compress.archivers.sevenz.Coders$DeflateDecoder,Class,No Comment,"static , final ",org.apache.commons.compress.archivers.sevenz.AbstractCoder,
org.apache.commons.compress.archivers.sevenz.Coders$DeflateDecoder$DeflateDecoderInputStream,Class,No Comment,"static , final ",java.io.FilterInputStream,
org.apache.commons.compress.archivers.sevenz.Coders$DeflateDecoder$DeflateDecoderOutputStream,Class,No Comment,"static , final ",java.io.OutputStream,
org.apache.commons.compress.archivers.sevenz.Coders$DeflateDecoder.DeflateDecoderInputStream,Class,No Comment,"static , final ",java.io.FilterInputStream,
org.apache.commons.compress.archivers.sevenz.Coders$DeflateDecoder.DeflateDecoderOutputStream,Class,No Comment,"static , final ",java.io.OutputStream,
org.apache.commons.compress.archivers.sevenz.SevenZMethodConfiguration,Class,* Combines a SevenZMethod with configuration options for the method.\n *\n * <p>\n * The exact type and interpretation of options depends on the method being configured. Currently supported are:\n * </p>\n *\n * <table>\n * <caption>Options</caption>\n * <tr>\n * <th>Method</th>\n * <th>Option Type</th>\n * <th>Description</th>\n * </tr>\n * <tr>\n * <td>BZIP2</td>\n * <td>Number</td>\n * <td>Block Size - an number between 1 and 9</td>\n * </tr>\n * <tr>\n * <td>DEFLATE</td>\n * <td>Number</td>\n * <td>Compression Level - an number between 1 and 9</td>\n * </tr>\n * <tr>\n * <td>LZMA2</td>\n * <td>Number</td>\n * <td>Dictionary Size - a number between 4096 and 768 MiB (768 &lt;&lt; 20)</td>\n * </tr>\n * <tr>\n * <td>LZMA2</td>\n * <td>org.tukaani.xz.LZMA2Options</td>\n * <td>Whole set of LZMA2 options.</td>\n * </tr>\n * <tr>\n * <td>DELTA_FILTER</td>\n * <td>Number</td>\n * <td>Delta Distance - a number between 1 and 256</td>\n * </tr>\n * </table>\n *\n * @Immutable\n * @since 1.8,public ,,
org.apache.commons.compress.archivers.sevenz.DeltaDecoder,Class,No Comment,final ,org.apache.commons.compress.archivers.sevenz.AbstractCoder,
org.apache.commons.compress.archivers.sevenz.BindPair,Class,No Comment,final ,,
org.apache.commons.compress.archivers.sevenz.CLI,Class,* Usage: archive-name [list],public ,,
org.apache.commons.compress.archivers.sevenz.CLI$Mode,Enum,* Enumerates modes.,private ,enum has no extends,
org.apache.commons.compress.archivers.sevenz.AbstractCoder,Abstract Class,* Abstracts a base Codec class.,abstract ,,
org.apache.commons.compress.archivers.sevenz.LZMADecoder,Class,No Comment,final ,org.apache.commons.compress.archivers.sevenz.AbstractCoder,
org.apache.commons.compress.archivers.sevenz.LZMA2Decoder,Class,No Comment,final ,org.apache.commons.compress.archivers.sevenz.AbstractCoder,
org.apache.commons.compress.archivers.sevenz.SevenZFile,Class,"* Reads a 7z file, using SeekableByteChannel under the covers.\n * <p>\n * The 7z file format is a flexible container that can contain many compression and encryption types, but at the moment only only Copy, LZMA, LZMA2, BZIP2,\n * Deflate and AES-256 + SHA-256 are supported.\n * </p>\n * <p>\n * The format is very Windows/Intel specific, so it uses little-endian byte order, doesn't store user/group or permission bits, and represents times using NTFS\n * timestamps (100 nanosecond units since 1 January 1601). Hence the official tools recommend against using it for backup purposes on *nix, and recommend\n * .tar.7z or .tar.lzma or .tar.xz instead.\n * </p>\n * <p>\n * Both the header and file contents may be compressed and/or encrypted. With both encrypted, neither file names nor file contents can be read, but the use of\n * encryption isn't plausibly deniable.\n * </p>\n * <p>\n * Multi volume archives can be read by concatenating the parts in correct order - either manually or by using {link\n * org.apache.commons.compress.utils.MultiReadOnlySeekableByteChannel} for example.\n * </p>\n *\n * @NotThreadSafe\n * @since 1.6",public ,,java.io.Closeable
org.apache.commons.compress.archivers.sevenz.SevenZFile$ArchiveStatistics,Class,No Comment,"private , static , final ",,
org.apache.commons.compress.archivers.sevenz.SevenZFile$Builder,Class,* Builds new instances of {@link SevenZFile}.\n     *\n     * @since 1.26.0,"public , static ",org.apache.commons.io.build.AbstractStreamBuilder,
org.apache.commons.compress.archivers.sevenz.BoundedSeekableByteChannelInputStream,Class,No Comment,final ,java.io.InputStream,
org.apache.commons.compress.archivers.sevenz.Folder,Class,* The unit of solid compression.,final ,,
org.apache.commons.compress.archivers.sevenz.Archive,Class,No Comment,final ,,
org.apache.commons.compress.archivers.sevenz.SevenZMethod,Enum,"* Enumerates the (partially) supported compression/encryption methods used in 7z archives.\n * <p>\n * All methods with a {@code _FILTER} suffix are used as preprocessors with the goal of creating a better compression ratio with the compressor that comes next\n * in the chain of methods. 7z will in general only allow them to be used together with a ""real"" compression method but Commons Compress doesn't enforce this.\n * </p>\n * <p>\n * The {@code BCJ_} filters work on executable files for the given platform and convert relative addresses to absolute addresses in CALL instructions. This\n * means they are only useful when applied to executables of the chosen platform.\n * </p>",public ,enum has no extends,
org.apache.commons.compress.archivers.sevenz.AES256Options,Class,* Options for {@link SevenZMethod#AES256SHA256} encoder\n *\n * @since 1.23\n * @see AES256SHA256Decoder,final ,,
org.apache.commons.compress.archivers.sevenz.SubStreamsInfo,Class,* Properties for non-empty files.,final ,,
org.apache.commons.compress.archivers.zip.ZipUtil,Abstract Class,* Utility class for handling DOS and Java time conversions.\n *\n * @Immutable,"public , abstract ",,
org.apache.commons.compress.archivers.zip.UnixStat,Interface,CheckStyle:InterfaceIsTypeCheck OFF - backward compatible,public ,,
org.apache.commons.compress.archivers.zip.ZipEncodingHelper,Abstract Class,* Static helper functions for robustly encoding file names in ZIP files.,"public , abstract ",,
org.apache.commons.compress.archivers.zip.X0014_X509Certificates,Class,"* PKCS#7 Store for X.509 Certificates (0x0014).\n *\n * <p>\n * This field MUST contain information about each of the certificates files may be signed with. When the Central Directory Encryption feature is enabled for a\n * ZIP file, this record will appear in the Archive Extra Data Record, otherwise it will appear in the first central directory record and will be ignored in any\n * other record.\n * </p>\n *\n * <p>\n * Note: all fields stored in Intel low-byte/high-byte order.\n * </p>\n *\n * <pre>\n *         Value     Size     Description\n *         -----     ----     -----------\n * (Store) 0x0014    2 bytes  Tag for this ""extra"" block type\n *         TSize     2 bytes  Size of the store data\n *         TData     TSize    Data about the store\n * </pre>\n *\n * @NotThreadSafe\n * @since 1.11",public ,org.apache.commons.compress.archivers.zip.PKWareExtraHeader,
org.apache.commons.compress.archivers.zip.AbstractUnicodeExtraField,Abstract Class,* A common base class for Unicode extra information extra fields.\n *\n * @NotThreadSafe,"public , abstract ",,org.apache.commons.compress.archivers.zip.ZipExtraField
org.apache.commons.compress.archivers.zip.ZipArchiveEntryRequest,Class,* A Thread-safe representation of a ZipArchiveEntry that is used to add entries to parallel archives.\n *\n * @since 1.10,public ,,
org.apache.commons.compress.archivers.zip.InflaterInputStreamWithStatistics,Class,* Helper class to provide statistics\n *\n * @since 1.17,default,java.util.zip.InflaterInputStream,org.apache.commons.compress.utils.InputStreamStatistics
org.apache.commons.compress.archivers.zip.AsiExtraField,Class,"* Adds Unix file permission and UID/GID fields as well as symbolic link handling.\n *\n * <p>\n * This class uses the ASi extra field in the format:\n * </p>\n *\n * <pre>\n *         Value         Size            Description\n *         -----         ----            -----------\n * (Unix3) 0x756e        Short           tag for this extra block type\n *         TSize         Short           total data size for this block\n *         CRC           Long            CRC-32 of the remaining data\n *         Mode          Short           file permissions\n *         SizDev        Long            symlink'd size OR major/minor dev num\n *         UID           Short           user ID\n *         GID           Short           group ID\n *         (var.)        variable        symbolic link file name\n * </pre>\n * <p>\n * taken from appnote.iz (Info-ZIP note, 981119) found at <a href=""ftp://ftp.uu.net/pub/archiving/zip/doc/"">ftp://ftp.uu.net/pub/archiving/zip/doc/</a>\n * </p>\n *\n * <p>\n * Short is two bytes and Long is four bytes in big-endian byte and word order, device numbers are currently not supported.\n * </p>\n *\n * @NotThreadSafe\n *\n *                <p>\n *                Since the documentation this class is based upon doesn't mention the character encoding of the file name at all, it is assumed that it uses\n *                the current platform's default encoding.\n *                </p>",public ,,org.apache.commons.compress.archivers.zip.ZipExtraField;org.apache.commons.compress.archivers.zip.UnixStat;java.lang.Cloneable
org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream,Class,"* Reimplementation of {@link java.util.zip.ZipOutputStream java.util.zip.ZipOutputStream} to handle the extended functionality of this package, especially\n * internal/external file attributes and extra fields with different layouts for local file data and central directory entries.\n * <p>\n * This class will try to use {@link java.nio.channels.SeekableByteChannel} when it knows that the output is going to go to a file and no split archive shall be\n * created.\n * </p>\n * <p>\n * If SeekableByteChannel cannot be used, this implementation will use a Data Descriptor to store size and CRC information for {@link #DEFLATED DEFLATED}\n * entries, you don't need to calculate them yourself. Unfortunately, this is not possible for the {@link #STORED STORED} method, where setting the CRC and\n * uncompressed size information is required before {@link #putArchiveEntry(ZipArchiveEntry)} can be called.\n * </p>\n * <p>\n * As of Apache Commons Compress 1.3, the class transparently supports Zip64 extensions and thus individual entries and archives larger than 4 GB or with more\n * than 65,536 entries in most cases but explicit control is provided via {@link #setUseZip64}. If the stream cannot use SeekableByteChannel and you try to\n * write a ZipArchiveEntry of unknown size, then Zip64 extensions will be disabled by default.\n * </p>\n *\n * @NotThreadSafe",public ,org.apache.commons.compress.archivers.ArchiveOutputStream,
org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream$CurrentEntry,Class,* Structure collecting information for the entry that is currently being written.,"private , static , final ",,
org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream$EntryMetaData,Class,No Comment,"private , static , final ",,
org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream$UnicodeExtraFieldPolicy,Class,* enum that represents the possible policies for creating Unicode extra fields.,"public , static , final ",,
org.apache.commons.compress.archivers.zip.SeekableChannelRandomAccessOutputStream,Class,Keep package-private; consider for Apache Commons IO.,final ,org.apache.commons.compress.archivers.zip.RandomAccessOutputStream,
org.apache.commons.compress.archivers.zip.X7875_NewUnix,Class,"* An extra field that stores Unix UID/GID data (owner &amp; group ownership) for a given ZIP entry. We're using the field definition given in Info-Zip's source\n * archive: zip-3.0.tar.gz/proginfo/extrafld.txt\n *\n * <pre>\n * Local-header version:\n *\n * Value         Size        Description\n * -----         ----        -----------\n * 0x7875        Short       tag for this extra block type (""ux"")\n * TSize         Short       total data size for this block\n * Version       1 byte      version of this extra field, currently 1\n * UIDSize       1 byte      Size of UID field\n * UID           Variable    UID for this entry (little-endian)\n * GIDSize       1 byte      Size of GID field\n * GID           Variable    GID for this entry (little-endian)\n *\n * Central-header version:\n *\n * Value         Size        Description\n * -----         ----        -----------\n * 0x7855        Short       tag for this extra block type (""Ux"")\n * TSize         Short       total data size for this block (0)\n * </pre>\n *\n * @since 1.5",public ,,org.apache.commons.compress.archivers.zip.ZipExtraField;java.lang.Cloneable;java.io.Serializable
org.apache.commons.compress.archivers.zip.X0019_EncryptionRecipientCertificateList,Class,"* PKCS#7 Encryption Recipient Certificate List (0x0019).\n *\n * <p>\n * This field MAY contain information about each of the certificates used in encryption processing and it can be used to identify who is allowed to decrypt\n * encrypted files. This field should only appear in the archive extra data record. This field is not required and serves only to aid archive modifications by\n * preserving public encryption key data. Individual security requirements may dictate that this data be omitted to deter information exposure.\n * </p>\n *\n * <p>\n * Note: all fields stored in Intel low-byte/high-byte order.\n * </p>\n *\n * <pre>\n *          Value     Size     Description\n *          -----     ----     -----------\n * (CStore) 0x0019    2 bytes  Tag for this ""extra"" block type\n *          TSize     2 bytes  Size of the store data\n *          Version   2 bytes  Format version number - must 0x0001 at this time\n *          CStore    (var)    PKCS#7 data blob\n * </pre>\n *\n * <p>\n * <b>See the section describing the Strong Encryption Specification for details. Refer to the section in this document entitled ""Incorporating PKWARE\n * Proprietary Technology into Your Product"" for more information.</b>\n * </p>\n *\n * @NotThreadSafe\n * @since 1.11",public ,org.apache.commons.compress.archivers.zip.PKWareExtraHeader,
org.apache.commons.compress.archivers.zip.Zip64ExtendedInformationExtraField,Class,"* Holds size and other extended information for entries that use Zip64 features.\n *\n * <p>\n * Currently Commons Compress doesn't support encrypting the central directory so the note in APPNOTE.TXT about masking doesn't apply.\n * </p>\n *\n * <p>\n * The implementation relies on data being read from the local file header and assumes that both size values are always present.\n * </p>\n *\n * @see <a href=""https://www.pkware.com/documents/casestudies/APPNOTE.TXT"">PKWARE APPNOTE.TXT, section 4.5.3</a>\n * @since 1.2\n * @NotThreadSafe",public ,,org.apache.commons.compress.archivers.zip.ZipExtraField
org.apache.commons.compress.archivers.zip.ZipLong,Class,* Utility class that represents a four byte integer with conversion rules for the little-endian byte order of ZIP files.\n *\n * @Immutable,"public , final ",,java.lang.Cloneable;java.io.Serializable
org.apache.commons.compress.archivers.zip.ResourceAlignmentExtraField,Class,"* An extra field who's sole purpose is to align and pad the local file header so that the entry's data starts at a certain position.\n *\n * <p>\n * The padding content of the padding is ignored and not retained when reading a padding field.\n * </p>\n *\n * <p>\n * This enables Commons Compress to create ""aligned"" archives similar to Android's {@code zipalign} command line tool.\n * </p>\n *\n * @since 1.14\n * @see ""https://developer.android.com/studio/command-line/zipalign.html""\n * @see ZipArchiveEntry#setAlignment",public ,,org.apache.commons.compress.archivers.zip.ZipExtraField
org.apache.commons.compress.archivers.zip.BinaryTree,Class,* Binary tree of positive values.\n *\n * @since 1.7,final ,,
org.apache.commons.compress.archivers.zip.JarMarker,Class,"* If this extra field is added as the very first extra field of the archive, Solaris will consider it an executable jar file.\n *\n * @Immutable","public , final ",,org.apache.commons.compress.archivers.zip.ZipExtraField
org.apache.commons.compress.archivers.zip.ScatterStatistics,Class,* Provides information about a scatter compression run.\n *\n * @since 1.10,public ,,
org.apache.commons.compress.archivers.zip.CharsetAccessor,Interface,"* An interface added to allow access to the character set associated with an {@link NioZipEncoding}, without requiring a new method to be added to\n * {@link ZipEncoding}.\n * <p>\n * This avoids introducing a potentially breaking change, or making {@link NioZipEncoding} a public class.\n * </p>\n *\n * @since 1.15",public ,,
org.apache.commons.compress.archivers.zip.X0017_StrongEncryptionHeader,Class,"* Strong Encryption Header (0x0017).\n *\n * <p>\n * Certificate-based encryption:\n * </p>\n *\n * <pre>\n * Value     Size     Description\n * -----     ----     -----------\n * 0x0017    2 bytes  Tag for this ""extra"" block type\n * TSize     2 bytes  Size of data that follows\n * Format    2 bytes  Format definition for this record\n * AlgID     2 bytes  Encryption algorithm identifier\n * Bitlen    2 bytes  Bit length of encryption key (32-448 bits)\n * Flags     2 bytes  Processing flags\n * RCount    4 bytes  Number of recipients.\n * HashAlg   2 bytes  Hash algorithm identifier\n * HSize     2 bytes  Hash size\n * SRList    (var)    Simple list of recipients hashed public keys\n *\n * Flags -   This defines the processing flags.\n * </pre>\n *\n * <ul>\n * <li>0x0007 - reserved for future use\n * <li>0x000F - reserved for future use\n * <li>0x0100 - Indicates non-OAEP key wrapping was used. If this this field is set, the version needed to extract must be at least 61. This means OAEP key\n * wrapping is not used when generating a Master Session Key using ErdData.\n * <li>0x4000 - ErdData must be decrypted using 3DES-168, otherwise use the same algorithm used for encrypting the file contents.\n * <li>0x8000 - reserved for future use\n * </ul>\n *\n * <pre>\n * RCount - This defines the number intended recipients whose\n *          public keys were used for encryption.  This identifies\n *          the number of elements in the SRList.\n *\n *          see also: reserved1\n *\n * HashAlg - This defines the hash algorithm used to calculate\n *           the public key hash of each public key used\n *           for encryption. This field currently supports\n *           only the following value for SHA-1\n *\n *           0x8004 - SHA1\n *\n * HSize -   This defines the size of a hashed public key.\n *\n * SRList -  This is a variable length list of the hashed\n *           public keys for each intended recipient.  Each\n *           element in this list is HSize.  The total size of\n *           SRList is determined using RCount * HSize.\n * </pre>\n *\n * <p>\n * Password-based Extra Field 0x0017 in central header only.\n * </p>\n *\n * <pre>\n * Value     Size     Description\n * -----     ----     -----------\n * 0x0017    2 bytes  Tag for this ""extra"" block type\n * TSize     2 bytes  Size of data that follows\n * Format    2 bytes  Format definition for this record\n * AlgID     2 bytes  Encryption algorithm identifier\n * Bitlen    2 bytes  Bit length of encryption key (32-448 bits)\n * Flags     2 bytes  Processing flags\n * (more?)\n * </pre>\n *\n * <p>\n * <strong>Format</strong> - the data format identifier for this record. The only value allowed at this time is the integer value 2.\n * </p>\n *\n * <p>\n * Password-based Extra Field 0x0017 preceding compressed file data.\n * </p>\n *\n * <pre>\n * Value     Size     Description\n * -----     ----     -----------\n * 0x0017    2 bytes  Tag for this ""extra"" block type\n * IVSize    2 bytes  Size of initialization vector (IV)\n * IVData    IVSize   Initialization vector for this file\n * Size      4 bytes  Size of remaining decryption header data\n * Format    2 bytes  Format definition for this record\n * AlgID     2 bytes  Encryption algorithm identifier\n * Bitlen    2 bytes  Bit length of encryption key (32-448 bits)\n * Flags     2 bytes  Processing flags\n * ErdSize   2 bytes  Size of Encrypted Random Data\n * ErdData   ErdSize  Encrypted Random Data\n * Reserved1 4 bytes  Reserved certificate processing data\n * Reserved2 (var)    Reserved for certificate processing data\n * VSize     2 bytes  Size of password validation data\n * VData     VSize-4  Password validation data\n * VCRC32    4 bytes  Standard ZIP CRC32 of password validation data\n *\n * IVData - The size of the IV should match the algorithm block size.\n *          The IVData can be completely random data.  If the size of\n *          the randomly generated data does not match the block size\n *          it should be complemented with zero's or truncated as\n *          necessary.  If IVSize is 0, then IV = CRC32 + Uncompressed\n *          File Size (as a 64 bit little-endian, unsigned integer value).\n *\n * Format -  the data format identifier for this record.  The only\n *           value allowed at this time is the integer value 2.\n *\n * ErdData - Encrypted random data is used to store random data that\n *           is used to generate a file session key for encrypting\n *           each file.  SHA1 is used to calculate hash data used to\n *           derive keys.  File session keys are derived from a master\n *           session key generated from the user-supplied password.\n *           If the Flags field in the decryption header contains\n *           the value 0x4000, then the ErdData field must be\n *           decrypted using 3DES. If the value 0x4000 is not set,\n *           then the ErdData field must be decrypted using AlgId.\n *\n * Reserved1 - Reserved for certificate processing, if value is\n *           zero, then Reserved2 data is absent.  See the explanation\n *           under the Certificate Processing Method for details on\n *           this data structure.\n *\n * Reserved2 - If present, the size of the Reserved2 data structure\n *           is located by skipping the first 4 bytes of this field\n *           and using the next 2 bytes as the remaining size.  See\n *           the explanation under the Certificate Processing Method\n *           for details on this data structure.\n *\n * VSize - This size value will always include the 4 bytes of the\n *         VCRC32 data and will be greater than 4 bytes.\n *\n * VData - Random data for password validation.  This data is VSize\n *         in length and VSize must be a multiple of the encryption\n *         block size.  VCRC32 is a checksum value of VData.\n *         VData and VCRC32 are stored encrypted and start the\n *         stream of encrypted data for a file.\n * </pre>\n *\n * <p>\n * Reserved1 - Certificate Decryption Header Reserved1 Data:\n * </p>\n *\n * <pre>\n * Value     Size     Description\n * -----     ----     -----------\n * RCount    4 bytes  Number of recipients.\n * </pre>\n *\n * <p>\n * RCount - This defines the number intended recipients whose public keys were used for encryption. This defines the number of elements in the REList field\n * defined below.\n * </p>\n *\n * <p>\n * Reserved2 - Certificate Decryption Header Reserved2 Data Structures:\n * </p>\n *\n * <pre>\n * Value     Size     Description\n * -----     ----     -----------\n * HashAlg   2 bytes  Hash algorithm identifier\n * HSize     2 bytes  Hash size\n * REList    (var)    List of recipient data elements\n *\n * HashAlg - This defines the hash algorithm used to calculate\n *           the public key hash of each public key used\n *           for encryption. This field currently supports\n *           only the following value for SHA-1\n *\n *               0x8004 - SHA1\n *\n * HSize -   This defines the size of a hashed public key\n *           defined in REHData.\n *\n * REList -  This is a variable length of list of recipient data.\n *           Each element in this list consists of a Recipient\n *           Element data structure as follows:\n * </pre>\n *\n * <p>\n * Recipient Element (REList) Data Structure:\n * </p>\n *\n * <pre>\n * Value     Size     Description\n * -----     ----     -----------\n * RESize    2 bytes  Size of REHData + REKData\n * REHData   HSize    Hash of recipients public key\n * REKData   (var)    Simple key blob\n *\n *\n * RESize -  This defines the size of an individual REList\n *           element.  This value is the combined size of the\n *           REHData field + REKData field.  REHData is defined by\n *           HSize.  REKData is variable and can be calculated\n *           for each REList element using RESize and HSize.\n *\n * REHData - Hashed public key for this recipient.\n *\n * REKData - Simple Key Blob.  The format of this data structure\n *           is identical to that defined in the Microsoft\n *           CryptoAPI and generated using the CryptExportKey()\n *           function.  The version of the Simple Key Blob\n *           supported at this time is 0x02 as defined by\n *           Microsoft.\n *\n *           For more details see https://msdn.microsoft.com/en-us/library/aa920051.aspx\n * </pre>\n *\n * <p>\n * <strong>Flags</strong> - Processing flags needed for decryption\n * </p>\n *\n * <ul>\n * <li>0x0001 - Password is required to decrypt</li>\n * <li>0x0002 - Certificates only</li>\n * <li>0x0003 - Password or certificate required to decrypt</li>\n * <li>0x0007 - reserved for future use\n * <li>0x000F - reserved for future use\n * <li>0x0100 - indicates non-OAEP key wrapping was used. If this field is set the version needed to extract must be at least 61. This means OAEP key wrapping\n * is not used when generating a Master Session Key using ErdData.\n * <li>0x4000 - ErdData must be decrypted using 3DES-168, otherwise use the same algorithm used for encrypting the file contents.\n * <li>0x8000 - reserved for future use.\n * </ul>\n *\n * <p>\n * <b>See the section describing the Strong Encryption Specification for details. Refer to the section in this document entitled ""Incorporating PKWARE\n * Proprietary Technology into Your Product"" for more information.</b>\n * </p>\n *\n * @NotThreadSafe\n * @since 1.11",public ,org.apache.commons.compress.archivers.zip.PKWareExtraHeader,
org.apache.commons.compress.archivers.zip.X000A_NTFS,Class,"* NTFS extra field that was thought to store various attributes but in reality only stores timestamps.\n *\n * <pre>\n *    4.5.5 -NTFS Extra Field (0x000a):\n *\n *       The following is the layout of the NTFS attributes\n *       ""extra"" block. (Note: At this time the Mtime, Atime\n *       and Ctime values MAY be used on any WIN32 system.)\n *\n *       Note: all fields stored in Intel low-byte/high-byte order.\n *\n *         Value      Size       Description\n *         -----      ----       -----------\n * (NTFS)  0x000a     2 bytes    Tag for this ""extra"" block type\n *         TSize      2 bytes    Size of the total ""extra"" block\n *         Reserved   4 bytes    Reserved for future use\n *         Tag1       2 bytes    NTFS attribute tag value #1\n *         Size1      2 bytes    Size of attribute #1, in bytes\n *         (var)      Size1      Attribute #1 data\n *          .\n *          .\n *          .\n *          TagN       2 bytes    NTFS attribute tag value #N\n *          SizeN      2 bytes    Size of attribute #N, in bytes\n *          (var)      SizeN      Attribute #N data\n *\n *        For NTFS, values for Tag1 through TagN are as follows:\n *        (currently only one set of attributes is defined for NTFS)\n *\n *          Tag        Size       Description\n *          -----      ----       -----------\n *          0x0001     2 bytes    Tag for attribute #1\n *          Size1      2 bytes    Size of attribute #1, in bytes\n *          Mtime      8 bytes    File last modification time\n *          Atime      8 bytes    File last access time\n *          Ctime      8 bytes    File creation time\n * </pre>\n *\n * @since 1.11\n * @NotThreadSafe",public ,,org.apache.commons.compress.archivers.zip.ZipExtraField
org.apache.commons.compress.archivers.zip.X0015_CertificateIdForFile,Class,"* X.509 Certificate ID and Signature for individual file (0x0015).\n *\n * <p>\n * This field contains the information about which certificate in the PKCS#7 store was used to sign a particular file. It also contains the signature data. This\n * field can appear multiple times, but can only appear once per certificate.\n * </p>\n *\n * <p>\n * Note: all fields stored in Intel low-byte/high-byte order.\n * </p>\n *\n * <pre>\n *         Value     Size     Description\n *         -----     ----     -----------\n * (CID)   0x0015    2 bytes  Tag for this ""extra"" block type\n *         TSize     2 bytes  Size of data that follows\n *         RCount    4 bytes  Number of recipients. (inferred)\n *         HashAlg   2 bytes  Hash algorithm identifier. (inferred)\n *         TData     TSize    Signature Data\n * </pre>\n *\n * @NotThreadSafe\n * @since 1.11",public ,org.apache.commons.compress.archivers.zip.PKWareExtraHeader,
org.apache.commons.compress.archivers.zip.ZipSplitOutputStream,Class,* Used internally by {@link ZipArchiveOutputStream} when creating a split archive.\n *\n * @since 1.20,final ,org.apache.commons.compress.archivers.zip.RandomAccessOutputStream,
org.apache.commons.compress.archivers.zip.ExtraFieldUtils,Class,CheckStyle:HideUtilityClassConstructorCheck OFF (bc),public ,,
org.apache.commons.compress.archivers.zip.ExtraFieldUtils$UnparseableExtraField,Class,"* ""enum"" for the possible actions to take if the extra field cannot be parsed.\n     * <p>\n     * This class has been created long before Java 5 and would have been a real enum ever since.\n     * </p>\n     *\n     * @since 1.1","public , static , final ",,org.apache.commons.compress.archivers.zip.UnparseableExtraFieldBehavior
org.apache.commons.compress.archivers.zip.UnparseableExtraFieldBehavior,Interface,* Handles extra field data that doesn't follow the recommended pattern for extra fields with a two-byte key and a two-byte length.\n *\n * @since 1.19,public ,,
org.apache.commons.compress.archivers.zip.UnrecognizedExtraField,Class,* Simple placeholder for all those extra fields we don't want to deal with.\n *\n * <p>\n * Assumes local file data and central directory entries are identical - unless told the opposite.\n * </p>\n *\n * @NotThreadSafe,public ,,org.apache.commons.compress.archivers.zip.ZipExtraField
org.apache.commons.compress.archivers.zip.FileRandomAccessOutputStream,Class,Keep package-private; consider for Apache Commons IO.,final ,org.apache.commons.compress.archivers.zip.RandomAccessOutputStream,
org.apache.commons.compress.archivers.zip.ZipArchiveEntryPredicate,Interface,* A predicate to test if a #ZipArchiveEntry matches a criteria. Some day this can extend java.util.function.Predicate\n *\n * @since 1.10,public ,,
org.apache.commons.compress.archivers.zip.ExplodingInputStream,Class,"* The implode compression method was added to PKZIP 1.01 released in 1989. It was then dropped from PKZIP 2.0 released in 1993 in favor of the deflate method.\n * <p>\n * The algorithm is described in the ZIP File Format Specification.\n *\n * @see <a href=""https://www.pkware.com/documents/casestudies/APPNOTE.TXT"">ZIP File Format Specification</a>\n * @since 1.7",final ,java.io.InputStream,org.apache.commons.compress.utils.InputStreamStatistics
org.apache.commons.compress.archivers.zip.CircularBuffer,Class,* Circular byte buffer.\n *\n * @since 1.7,final ,,
org.apache.commons.compress.archivers.zip.UnshrinkingInputStream,Class,"* Input stream that decompresses ZIP method 1 (unshrinking). A variation of the LZW algorithm, with some twists.\n *\n * @NotThreadSafe\n * @since 1.7",final ,org.apache.commons.compress.compressors.lzw.LZWInputStream,
org.apache.commons.compress.archivers.zip.X5455_ExtendedTimestamp,Class,"* <p>\n * An extra field that stores additional file and directory timestamp data for ZIP entries. Each ZIP entry can include up to three timestamps (modify, access,\n * create*). The timestamps are stored as 32 bit signed integers representing seconds since Unix epoch (Jan 1st, 1970, UTC). This field improves on ZIP's\n * default timestamp granularity, since it allows one to store additional timestamps, and, in addition, the timestamps are stored using per-second granularity\n * (zip's default behavior can only store timestamps to the nearest <em>even</em> second).\n * </p>\n * <p>\n * Unfortunately, 32 (signed) bits can only store dates up to the year 2037, and so this extra field will eventually be obsolete. Enjoy it while it lasts!\n * </p>\n * <ul>\n * <li><strong>modifyTime:</strong> most recent time of file/directory modification (or file/dir creation if the entry has not been modified since it was\n * created).</li>\n * <li><strong>accessTime:</strong> most recent time file/directory was opened (for example, read from disk). Many people disable their operating systems from\n * updating this value using the NOATIME mount option to optimize disk behavior, and thus it's not always reliable. In those cases it's always equal to\n * modifyTime.</li>\n * <li><strong>*createTime:</strong> modern Linux file systems (for example, ext2 and newer) do not appear to store a value like this, and so it's usually\n * omitted altogether in the ZIP extra field. Perhaps other Unix systems track this.</li>\n * </ul>\n * <p>\n * We're using the field definition given in Info-Zip's source archive: zip-3.0.tar.gz/proginfo/extrafld.txt\n * </p>\n *\n * <pre>\n * Value         Size        Description\n * -----         ----        -----------\n * 0x5455        Short       tag for this extra block type (""UT"")\n * TSize         Short       total data size for this block\n * Flags         Byte        info bits\n * (ModTime)     Long        time of last modification (UTC/GMT)\n * (AcTime)      Long        time of last access (UTC/GMT)\n * (CrTime)      Long        time of original creation (UTC/GMT)\n *\n * Central-header version:\n *\n * Value         Size        Description\n * -----         ----        -----------\n * 0x5455        Short       tag for this extra block type (""UT"")\n * TSize         Short       total data size for this block\n * Flags         Byte        info bits (refers to local header!)\n * (ModTime)     Long        time of last modification (UTC/GMT)\n * </pre>\n *\n * @since 1.5",public ,,org.apache.commons.compress.archivers.zip.ZipExtraField;java.lang.Cloneable;java.io.Serializable
org.apache.commons.compress.archivers.zip.ZipArchiveEntryRequestSupplier,Interface,"* Supplies {@link ZipArchiveEntryRequest}.\n *\n * Implementations are required to support thread-handover. While an instance will not be accessed concurrently by multiple threads, it will be called by a\n * different thread than it was created on.\n *\n * @since 1.13",public ,,
org.apache.commons.compress.archivers.zip.PKWareExtraHeader,Abstract Class,* Base class for all PKWare strong crypto extra headers.\n *\n * <p>\n * This base class acts as a marker so you know you can ignore all extra fields that extend this class if you are not interested in the meta data of PKWare\n * strong encryption.\n * </p>\n *\n * <strong>Algorithm IDs</strong> - integer identifier of the encryption algorithm from the following range\n *\n * <ul>\n * <li>0x6601 - DES</li>\n * <li>0x6602 - RC2 (version needed to extract &lt; 5.2)</li>\n * <li>0x6603 - 3DES 168</li>\n * <li>0x6609 - 3DES 112</li>\n * <li>0x660E - AES 128</li>\n * <li>0x660F - AES 192</li>\n * <li>0x6610 - AES 256</li>\n * <li>0x6702 - RC2 (version needed to extract &gt;= 5.2)</li>\n * <li>0x6720 - Blowfish</li>\n * <li>0x6721 - Twofish</li>\n * <li>0x6801 - RC4</li>\n * <li>0xFFFF - Unknown algorithm</li>\n * </ul>\n *\n * <strong>Hash Algorithms</strong> - integer identifier of the hash algorithm from the following range\n *\n * <ul>\n * <li>0x0000 - none</li>\n * <li>0x0001 - CRC32</li>\n * <li>0x8003 - MD5</li>\n * <li>0x8004 - SHA1</li>\n * <li>0x8007 - RIPEMD160</li>\n * <li>0x800C - SHA256</li>\n * <li>0x800D - SHA384</li>\n * <li>0x800E - SHA512</li>\n * </ul>\n *\n * @since 1.11,"public , abstract ",,org.apache.commons.compress.archivers.zip.ZipExtraField
org.apache.commons.compress.archivers.zip.PKWareExtraHeader$EncryptionAlgorithm,Enum,* Enumerates encryption algorithm.\n     *\n     * @since 1.11,public ,enum has no extends,
org.apache.commons.compress.archivers.zip.PKWareExtraHeader$HashAlgorithm,Enum,* Enumerates hash Algorithm\n     *\n     * @since 1.11,public ,enum has no extends,
org.apache.commons.compress.archivers.zip.ZipFile,Class,"* Replacement for {@link java.util.zip.ZipFile}.\n * <p>\n * This class adds support for file name encodings other than UTF-8 (which is required to work on ZIP files created by native ZIP tools and is able to skip a\n * preamble like the one found in self extracting archives. Furthermore it returns instances of\n * {@code org.apache.commons.compress.archivers.zip.ZipArchiveEntry} instead of {@link java.util.zip.ZipEntry}.\n * </p>\n * <p>\n * It doesn't extend {@link java.util.zip.ZipFile} as it would have to reimplement all methods anyway. Like {@link java.util.zip.ZipFile}, it uses\n * SeekableByteChannel under the covers and supports compressed and uncompressed entries. As of Apache Commons Compress 1.3 it also transparently supports Zip64\n * extensions and thus individual entries and archives larger than 4 GB or with more than 65,536 entries.\n * </p>\n * <p>\n * The method signatures mimic the ones of {@link java.util.zip.ZipFile}, with a couple of exceptions:\n * </p>\n * <ul>\n * <li>There is no getName method.</li>\n * <li>entries has been renamed to getEntries.</li>\n * <li>getEntries and getEntry return {@code org.apache.commons.compress.archivers.zip.ZipArchiveEntry} instances.</li>\n * <li>close is allowed to throw IOException.</li>\n * </ul>",public ,,java.io.Closeable
org.apache.commons.compress.archivers.zip.ZipFile$BoundedFileChannelInputStream,Class,* Lock-free implementation of BoundedInputStream. The implementation uses positioned reads on the underlying archive file channel and therefore performs\n     * significantly faster in concurrent environment.,"private , static , final ",org.apache.commons.compress.utils.BoundedArchiveInputStream,
org.apache.commons.compress.archivers.zip.ZipFile$Builder,Class,"* Builds new {@link ZipFile} instances.\n     * <p>\n     * The channel will be opened for reading, assuming the specified encoding for file names.\n     * </p>\n     * <p>\n     * See {@link org.apache.commons.compress.utils.SeekableInMemoryByteChannel} to read from an in-memory archive.\n     * </p>\n     * <p>\n     * By default the central directory record and all local file headers of the archive will be read immediately which may take a considerable amount of time\n     * when the archive is big. The {@code ignoreLocalFileHeader} parameter can be set to {@code true} which restricts parsing to the central directory.\n     * Unfortunately the local file header may contain information not present inside of the central directory which will not be available when the argument is\n     * set to {@code true}. This includes the content of the Unicode extra field, so setting {@code\n     * ignoreLocalFileHeader} to {@code true} means {@code useUnicodeExtraFields} will be ignored effectively.\n     * </p>\n     *\n     * @since 1.26.0","public , static ",org.apache.commons.io.build.AbstractStreamBuilder,
org.apache.commons.compress.archivers.zip.ZipFile$Entry,Class,* Extends ZipArchiveEntry to store the offset within the archive.,"private , static , final ",org.apache.commons.compress.archivers.zip.ZipArchiveEntry,
org.apache.commons.compress.archivers.zip.ZipFile$NameAndComment,Class,No Comment,"private , static , final ",,
org.apache.commons.compress.archivers.zip.ZipFile$StoredStatisticsStream,Class,No Comment,"private , static , final ",org.apache.commons.io.input.BoundedInputStream,org.apache.commons.compress.utils.InputStreamStatistics
org.apache.commons.compress.archivers.zip.GeneralPurposeBit,Class,"* Parser/encoder for the ""general purpose bit"" field in ZIP's local file and central directory headers.\n *\n * @since 1.1\n * @NotThreadSafe","public , final ",,java.lang.Cloneable
org.apache.commons.compress.archivers.zip.X0016_CertificateIdForCentralDirectory,Class,"* X.509 Certificate ID and Signature for central directory (0x0016).\n *\n * <p>\n * This field contains the information about which certificate in the PKCS#7 store was used to sign the central directory structure. When the Central Directory\n * Encryption feature is enabled for a ZIP file, this record will appear in the Archive Extra Data Record, otherwise it will appear in the first central\n * directory record.\n * </p>\n *\n * <p>\n * Note: all fields stored in Intel low-byte/high-byte order.\n * </p>\n *\n * <pre>\n *         Value     Size     Description\n *         -----     ----     -----------\n * (CDID)  0x0016    2 bytes  Tag for this ""extra"" block type\n *         TSize     2 bytes  Size of data that follows\n *         RCount    4 bytes  Number of recipients. (inferred)\n *         HashAlg   2 bytes  Hash algorithm identifier. (inferred)\n *         TData     TSize    Data\n * </pre>\n *\n * @NotThreadSafe\n * @since 1.11",public ,org.apache.commons.compress.archivers.zip.PKWareExtraHeader,
org.apache.commons.compress.archivers.zip.ZipShort,Class,* Utility class that represents a two byte integer with conversion rules for the little-endian byte order of ZIP files.\n *\n * @Immutable,"public , final ",,java.lang.Cloneable;java.io.Serializable
org.apache.commons.compress.archivers.zip.ZipExtraField,Interface,"* General format of extra field data.\n *\n * <p>\n * Extra fields usually appear twice per file, once in the local file data and once in the central directory. Usually they are the same, but they don't have to\n * be. {@link java.util.zip.ZipOutputStream java.util.zip.ZipOutputStream} will only use the local file data in both places.\n * </p>",public ,,
org.apache.commons.compress.archivers.zip.ZipConstants,Class,* Various constants used throughout the package.\n *\n * @since 1.3,final ,,
org.apache.commons.compress.archivers.zip.ZipIoUtil,Class,Keep package-private; consider for Apache Commons IO.,final ,,
org.apache.commons.compress.archivers.zip.RandomAccessOutputStream,Abstract Class,Keep package-private; consider for Apache Commons IO.,abstract ,java.io.OutputStream,
org.apache.commons.compress.archivers.zip.UnicodePathExtraField,Class,"* Info-ZIP Unicode Path Extra Field (0x7075):\n *\n * <p>\n * Stores the UTF-8 version of the file name field as stored in the local header and central directory header.\n * </p>\n *\n * @see <a href=""https://www.pkware.com/documents/casestudies/APPNOTE.TXT"">PKWARE APPNOTE.TXT, section 4.6.9</a>\n * @NotThreadSafe super-class is not thread-safe",public ,org.apache.commons.compress.archivers.zip.AbstractUnicodeExtraField,
org.apache.commons.compress.archivers.zip.ExtraFieldParsingBehavior,Interface,* Controls details of parsing ZIP extra fields.\n *\n * @since 1.19,public ,org.apache.commons.compress.archivers.zip.UnparseableExtraFieldBehavior,
org.apache.commons.compress.archivers.zip.NioZipEncoding,Class,"* A ZipEncoding, which uses a {@link Charset} to encode names.\n * <p>\n * The methods of this class are reentrant.\n * </p>\n *\n * @Immutable",final ,,org.apache.commons.compress.archivers.zip.ZipEncoding;org.apache.commons.compress.archivers.zip.CharsetAccessor
org.apache.commons.compress.archivers.zip.ParallelScatterZipCreator,Class,"* Creates a ZIP in parallel by using multiple threadlocal {@link ScatterZipOutputStream} instances.\n * <p>\n * Note that until 1.18, this class generally made no guarantees about the order of things written to the output file. Things that needed to come in a specific\n * order (manifests, directories) had to be handled by the client of this class, usually by writing these things to the {@link ZipArchiveOutputStream}\n * <em>before</em> calling {@link #writeTo writeTo} on this class.\n * </p>\n * <p>\n * The client can supply an {@link java.util.concurrent.ExecutorService}, but for reasons of memory model consistency, this will be shut down by this class\n * prior to completion.\n * </p>\n *\n * @since 1.10",public ,,
org.apache.commons.compress.archivers.zip.ZipArchiveEntry,Class,"* Extension that adds better handling of extra fields and provides access to the internal and external file attributes.\n *\n * <p>\n * The extra data is expected to follow the recommendation of <a href=""http://www.pkware.com/documents/casestudies/APPNOTE.TXT"">APPNOTE.TXT</a>:\n * </p>\n * <ul>\n * <li>the extra byte array consists of a sequence of extra fields</li>\n * <li>each extra fields starts by a two byte header id followed by a two byte sequence holding the length of the remainder of data.</li>\n * </ul>\n *\n * <p>\n * Any extra data that cannot be parsed by the rules above will be consumed as ""unparseable"" extra data and treated differently by the methods of this class.\n * Versions prior to Apache Commons Compress 1.1 would have thrown an exception if any attempt was made to read or write extra data not conforming to the\n * recommendation.\n * </p>\n *\n * @NotThreadSafe",public ,java.util.zip.ZipEntry,org.apache.commons.compress.archivers.ArchiveEntry;org.apache.commons.compress.archivers.EntryStreamOffsets
org.apache.commons.compress.archivers.zip.ZipArchiveEntry$CommentSource,Enum,* Enumerates how the comment of this entry has been determined.\n     *\n     * @since 1.16,public ,enum has no extends,
org.apache.commons.compress.archivers.zip.ZipArchiveEntry$ExtraFieldParsingMode,Enum,"* Enumerates how to try to parse the extra fields.\n     *\n     * <p>\n     * Configures the behavior for:\n     * </p>\n     * <ul>\n     * <li>What shall happen if the extra field content doesn't follow the recommended pattern of two-byte id followed by a two-byte length?</li>\n     * <li>What shall happen if an extra field is generally supported by Commons Compress but its content cannot be parsed correctly? This may for example\n     * happen if the archive is corrupt, it triggers a bug in Commons Compress or the extra field uses a version not (yet) supported by Commons Compress.</li>\n     * </ul>\n     *\n     * @since 1.19",public ,enum has no extends,org.apache.commons.compress.archivers.zip.ExtraFieldParsingBehavior
org.apache.commons.compress.archivers.zip.ZipArchiveEntry$NameSource,Enum,* Indicates how the name of this entry has been determined.\n     *\n     * @since 1.16,public ,enum has no extends,
org.apache.commons.compress.archivers.zip.ZipEncoding,Interface,"* An interface for encoders that do a pretty encoding of ZIP file names.\n *\n * <p>\n * There are mostly two implementations, one that uses java.nio {@link java.nio.charset.Charset Charset} and one implementation, which copes with simple 8 bit\n * charsets, because java-1.4 did not support Cp437 in java.nio.\n * </p>\n *\n * <p>\n * The main reason for defining an own encoding layer comes from the problems with {@link String#getBytes(String) String.getBytes}, which encodes\n * unknown characters as ASCII quotation marks ('?'). Quotation marks are per definition an invalid file name on some operating systems like Windows, which\n * leads to ignored ZIP entries.\n * </p>\n *\n * <p>\n * All implementations should implement this interface in a reentrant way.\n * </p>",public ,,
org.apache.commons.compress.archivers.zip.UnsupportedZipFeatureException,Class,* Exception thrown when attempting to read or write data for a ZIP entry that uses ZIP features not supported by this library.\n *\n * @since 1.1,public ,java.util.zip.ZipException,
org.apache.commons.compress.archivers.zip.UnsupportedZipFeatureException$Feature,Class,* ZIP Features that may or may not be supported.\n     *\n     * @since 1.1,"public , static ",,java.io.Serializable
org.apache.commons.compress.archivers.zip.UnicodeCommentExtraField,Class,"* Info-ZIP Unicode Comment Extra Field (0x6375):\n *\n * <p>\n * Stores the UTF-8 version of the file comment as stored in the central directory header.\n * </p>\n *\n * @see <a href=""https://www.pkware.com/documents/casestudies/APPNOTE.TXT"">PKWARE APPNOTE.TXT, section 4.6.8</a>\n * @NotThreadSafe super-class is not thread-safe",public ,org.apache.commons.compress.archivers.zip.AbstractUnicodeExtraField,
org.apache.commons.compress.archivers.zip.DefaultBackingStoreSupplier,Class,"* Implements {@link ScatterGatherBackingStoreSupplier} using a temporary folder.\n * <p>\n * For example:\n * </p>\n *\n * <pre>\n * final Path dir = Paths.get(""target/custom-temp-dir"");\n * Files.createDirectories(dir);\n * final ParallelScatterZipCreator zipCreator = new ParallelScatterZipCreator(Executors.newFixedThreadPool(Runtime.getRuntime().availableProcessors()),\n *         new DefaultBackingStoreSupplier(dir));\n * </pre>\n *\n * @since 1.23",public ,,org.apache.commons.compress.parallel.ScatterGatherBackingStoreSupplier
org.apache.commons.compress.archivers.zip.ZipMethod,Enum,* Enumerates known compression methods.\n *\n * Some of these methods are currently not supported by commons compress.\n *\n * @since 1.5,public ,enum has no extends,
org.apache.commons.compress.archivers.zip.ZipArchiveInputStream,Class,"* Implements an input stream that can read Zip archives.\n * <p>\n * As of Apache Commons Compress it transparently supports Zip64 extensions and thus individual entries and archives larger than 4 GB or with more than 65,536\n * entries.\n * </p>\n * <p>\n * The {@link ZipFile} class is preferred when reading from files as {@link ZipArchiveInputStream} is limited by not being able to read the central directory\n * header before returning entries. In particular {@link ZipArchiveInputStream}\n * </p>\n * <ul>\n * <li>may return entries that are not part of the central directory at all and shouldn't be considered part of the archive.</li>\n * <li>may return several entries with the same name.</li>\n * <li>will not return internal or external attributes.</li>\n * <li>may return incomplete extra field data.</li>\n * <li>may return unknown sizes and CRC values for entries until the next entry has been reached if the archive uses the data descriptor feature.</li>\n * </ul>\n *\n * @see ZipFile\n * @NotThreadSafe",public ,org.apache.commons.compress.archivers.ArchiveInputStream,org.apache.commons.compress.utils.InputStreamStatistics
org.apache.commons.compress.archivers.zip.ZipArchiveInputStream$BoundCountInputStream,Class,* Input stream adapted from commons-io.,"private , final ",org.apache.commons.io.input.BoundedInputStream,
org.apache.commons.compress.archivers.zip.ZipArchiveInputStream$CurrentEntry,Class,* Structure collecting information for the entry that is currently being read.,"private , static , final ",,
org.apache.commons.compress.archivers.zip.ScatterZipOutputStream,Class,"* A ZIP output stream that is optimized for multi-threaded scatter/gather construction of ZIP files.\n * <p>\n * The internal data format of the entries used by this class are entirely private to this class and are not part of any public api whatsoever.\n * </p>\n * <p>\n * It is possible to extend this class to support different kinds of backing storage, the default implementation only supports file-based backing.\n * </p>\n * <p>\n * Thread safety: This class supports multiple threads. But the ""writeTo"" method must be called by the thread that originally created the\n * {@link ZipArchiveEntry}.\n * </p>\n *\n * @since 1.10",public ,,java.io.Closeable
org.apache.commons.compress.archivers.zip.ScatterZipOutputStream$CompressedEntry,Class,No Comment,"private , static , final ",,
org.apache.commons.compress.archivers.zip.ScatterZipOutputStream$ZipEntryWriter,Class,No Comment,"public , static ",,java.io.Closeable
org.apache.commons.compress.archivers.zip.Zip64RequiredException,Class,* Exception thrown when attempting to write data that requires Zip64 support to an archive and {@link ZipArchiveOutputStream#setUseZip64 UseZip64} has been set\n * to {@link Zip64Mode#Never Never}.\n *\n * @since 1.3,public ,java.util.zip.ZipException,
org.apache.commons.compress.archivers.zip.ZipEightByteInteger,Class,* Utility class that represents an eight byte integer with conversion rules for the little-endian byte order of ZIP files.\n *\n * @Immutable\n * @since 1.2,"public , final ",,java.io.Serializable
org.apache.commons.compress.archivers.zip.UnparseableExtraFieldData,Class,"* Wrapper for extra field data that doesn't conform to the recommended format of header-tag + size + data.\n *\n * <p>\n * The header-id is artificial (and not listed as a known ID in <a href=""https://www.pkware.com/documents/casestudies/APPNOTE.TXT"">APPNOTE.TXT</a>). Since it\n * isn't used anywhere except to satisfy the ZipExtraField contract it shouldn't matter anyway.\n * </p>\n *\n * @since 1.1\n * @NotThreadSafe","public , final ",,org.apache.commons.compress.archivers.zip.ZipExtraField
org.apache.commons.compress.archivers.zip.StreamCompressor,Abstract Class,"* Encapsulates a {@link Deflater} and CRC calculator, handling multiple types of output streams. Currently {@link java.util.zip.ZipEntry#DEFLATED} and\n * {@link java.util.zip.ZipEntry#STORED} are the only supported compression methods.\n *\n * @since 1.10","public , abstract ",,java.io.Closeable
org.apache.commons.compress.archivers.zip.StreamCompressor$DataOutputCompressor,Class,No Comment,"private , static , final ",org.apache.commons.compress.archivers.zip.StreamCompressor,
org.apache.commons.compress.archivers.zip.StreamCompressor$OutputStreamCompressor,Class,No Comment,"private , static , final ",org.apache.commons.compress.archivers.zip.StreamCompressor,
org.apache.commons.compress.archivers.zip.StreamCompressor$ScatterGatherBackingStoreCompressor,Class,No Comment,"private , static , final ",org.apache.commons.compress.archivers.zip.StreamCompressor,
org.apache.commons.compress.archivers.zip.StreamCompressor$SeekableByteChannelCompressor,Class,No Comment,"private , static , final ",org.apache.commons.compress.archivers.zip.StreamCompressor,
org.apache.commons.compress.archivers.zip.Zip64Mode,Enum,* Enumerates the different modes {@link ZipArchiveOutputStream} can operate in.\n *\n * @see ZipArchiveOutputStream#setUseZip64\n * @since 1.3,public ,enum has no extends,
org.apache.commons.compress.archivers.zip.ZipSplitReadOnlySeekableByteChannel,Class,* {@link MultiReadOnlySeekableByteChannel} that knows what a split ZIP archive should look like.\n * <p>\n * If you want to read a split archive using {@link ZipFile} then create an instance of this class from the parts of the archive.\n * </p>\n *\n * @since 1.20,public ,org.apache.commons.compress.utils.MultiReadOnlySeekableByteChannel,
org.apache.commons.compress.archivers.zip.ZipSplitReadOnlySeekableByteChannel$ZipSplitSegmentComparator,Class,No Comment,"private , static , final ",,java.util.Comparator;java.io.Serializable
org.apache.commons.compress.archivers.zip.BitStream,Class,* Iterates over the bits of an InputStream. For each byte the bits are read from the right to the left.\n *\n * @since 1.7,final ,org.apache.commons.compress.utils.BitInputStream,
org.apache.commons.compress.archivers.examples.Expander,Class,* Provides a high level API for expanding archives.\n *\n * @since 1.17,public ,,
org.apache.commons.compress.archivers.examples.Expander$ArchiveEntryBiConsumer,Interface,No Comment,private ,,
org.apache.commons.compress.archivers.examples.Expander$ArchiveEntrySupplier,Interface,No Comment,private ,,
org.apache.commons.compress.archivers.examples.CloseableConsumerAdapter,Class,No Comment,final ,,java.io.Closeable
org.apache.commons.compress.archivers.examples.CloseableConsumer,Interface,* Callback that is informed about a closable resource that has been wrapped around a passed in stream or channel by Expander or Archiver when Expander or\n * Archiver no longer need them.\n *\n * <p>\n * This provides a way to close said resources in the calling code.\n * </p>\n *\n * @since 1.19,public ,,
org.apache.commons.compress.archivers.examples.Archiver,Class,* Provides a high level API for creating archives.\n *\n * @since 1.17\n * @since 1.21 Supports {@link Path}.,public ,,
org.apache.commons.compress.archivers.examples.Archiver$ArchiverFileVisitor,Class,No Comment,"private , static ",java.nio.file.SimpleFileVisitor,
org.apache.commons.compress.archivers.ar.ArArchiveEntry,Class,"* Represents an archive entry in the ""ar"" format.\n * <p>\n * Each AR archive starts with ""!&lt;arch&gt;"" followed by a LF. After these 8 bytes the archive entries are listed. The format of an entry header is as it\n * follows:\n * </p>\n *\n * <pre>\n * START BYTE   END BYTE    NAME                    FORMAT      LENGTH\n * 0            15          File name               ASCII       16\n * 16           27          Modification timestamp  Decimal     12\n * 28           33          Owner ID                Decimal     6\n * 34           39          Group ID                Decimal     6\n * 40           47          File mode               Octal       8\n * 48           57          File size (bytes)       Decimal     10\n * 58           59          File magic              \140\012    2\n * </pre>\n * <p>\n * This specifies that an ar archive entry header contains 60 bytes.\n * </p>\n * <p>\n * Due to the limitation of the file name length to 16 bytes GNU and BSD has their own variants of this format. Currently Commons Compress can read but not\n * write the GNU variant. It fully supports the BSD variant.\n * </p>\n *\n * @see <a href=""https://www.freebsd.org/cgi/man.cgi?query=ar&sektion=5"">ar man page</a>\n * @Immutable",public ,,org.apache.commons.compress.archivers.ArchiveEntry
org.apache.commons.compress.archivers.ar.ArArchiveOutputStream,Class,"* Implements the ""ar"" archive format as an output stream.\n *\n * @NotThreadSafe",public ,org.apache.commons.compress.archivers.ArchiveOutputStream,
org.apache.commons.compress.archivers.ar.ArArchiveInputStream,Class,"* Implements the ""ar"" archive format as an input stream.\n *\n * @NotThreadSafe",public ,org.apache.commons.compress.archivers.ArchiveInputStream,
org.apache.commons.compress.archivers.EntryStreamOffsets,Interface,* Provides information about ArchiveEntry stream offsets.,public ,,
org.apache.commons.compress.archivers.Lister,Class,"* Simple command line application that lists the contents of an archive.\n *\n * <p>\n * The name of the archive must be given as a command line argument.\n * </p>\n * <p>\n * The optional second argument defines the archive type, in case the format is not recognized.\n * </p>\n *\n * @since 1.1","public , final ",,
org.apache.commons.compress.archivers.arj.MainHeader,Class,No Comment,final ,,
org.apache.commons.compress.archivers.arj.MainHeader$Flags,Class,No Comment,"static , final ",,
org.apache.commons.compress.archivers.arj.MainHeader$HostOS,Class,No Comment,"static , final ",,
org.apache.commons.compress.archivers.arj.ArjArchiveEntry,Class,* An entry in an ARJ archive.\n *\n * @NotThreadSafe\n * @since 1.6,public ,,org.apache.commons.compress.archivers.ArchiveEntry
org.apache.commons.compress.archivers.arj.ArjArchiveEntry$HostOs,Class,* The known values for HostOs.,"public , static ",,
org.apache.commons.compress.archivers.arj.ArjArchiveInputStream,Class,"* Implements the ""arj"" archive format as an InputStream.\n * <ul>\n * <li><a href=""https://github.com/FarGroup/FarManager/blob/master/plugins/multiarc/arc.doc/arj.txt"">Reference 1</a></li>\n * <li><a href=""http://www.fileformat.info/format/arj/corion.htm"">Reference 2</a></li>\n * </ul>\n *\n * @NotThreadSafe\n * @since 1.6",public ,org.apache.commons.compress.archivers.ArchiveInputStream,
org.apache.commons.compress.archivers.arj.LocalFileHeader,Class,No Comment,final ,,
org.apache.commons.compress.archivers.arj.LocalFileHeader$FileTypes,Class,No Comment,"static , final ",,
org.apache.commons.compress.archivers.arj.LocalFileHeader$Flags,Class,No Comment,"static , final ",,
org.apache.commons.compress.archivers.arj.LocalFileHeader$Methods,Class,No Comment,"static , final ",,
org.apache.commons.compress.archivers.ArchiveEntry,Interface,* An entry of an archive.,public ,,
org.apache.commons.compress.archivers.tar.TarArchiveInputStream,Class,"* The TarInputStream reads a Unix tar archive as an InputStream. methods are provided to position at each successive entry in the archive, and the read each\n * entry as a normal input stream using read().\n *\n * @NotThreadSafe",public ,org.apache.commons.compress.archivers.ArchiveInputStream,
org.apache.commons.compress.archivers.tar.TarArchiveOutputStream,Class,"* The TarOutputStream writes a Unix tar archive as an OutputStream. Methods are provided to put entries, and then write their contents by writing to this\n * stream using write().\n *\n * <p>\n * tar archives consist of a sequence of records of 512 bytes each that are grouped into blocks. Prior to Apache Commons Compress 1.14 it has been possible to\n * configure a record size different from 512 bytes and arbitrary block sizes. Starting with Compress 1.15 512 is the only valid option for the record size and\n * the block size must be a multiple of 512. Also the default block size changed from 10240 bytes prior to Compress 1.15 to 512 bytes with Compress 1.15.\n * </p>\n *\n * @NotThreadSafe",public ,org.apache.commons.compress.archivers.ArchiveOutputStream,
org.apache.commons.compress.archivers.tar.TarFile,Class,* Provides random access to Unix archives.\n *\n * @since 1.21,public ,,java.io.Closeable
org.apache.commons.compress.archivers.tar.TarFile$BoundedTarEntryInputStream,Class,No Comment,"private , final ",org.apache.commons.compress.utils.BoundedArchiveInputStream,
org.apache.commons.compress.archivers.tar.TarConstants,Interface,CheckStyle:InterfaceIsTypeCheck OFF (bc),public ,,
org.apache.commons.compress.archivers.tar.TarArchiveSparseZeroInputStream,Class,"* This is an InputStream that always return 0, this is used when reading the ""holes"" of a sparse file",final ,java.io.InputStream,
org.apache.commons.compress.archivers.tar.TarArchiveSparseEntry,Class,No Comment,public ,,org.apache.commons.compress.archivers.tar.TarConstants
org.apache.commons.compress.archivers.tar.TarGnuSparseKeys,Class,* GNU sparse key names.,final ,,
org.apache.commons.compress.archivers.tar.TarArchiveStructSparse,Class,"* A {@code struct sparse} in a <a href=""https://www.gnu.org/software/tar/manual/html_node/Standard.html"">Tar archive</a>.\n * <p>\n * Whereas, ""struct sparse"" is:\n * </p>\n * <pre>\n * struct sparse {\n * char offset[12];   // offset 0\n * char numbytes[12]; // offset 12\n * };\n * </pre>\n *\n * @since 1.20","public , final ",,
org.apache.commons.compress.archivers.tar.TarArchiveEntry,Class,"* An entry in a <a href=""https://www.gnu.org/software/tar/manual/html_node/Standard.html"">Tar archive</a>.\n * It consists of the entry's header, as well as the entry's File. Entries can be instantiated in one of three\n * ways, depending on how they are to be used.\n * <p>\n * TarEntries that are created from the header bytes read from an archive are instantiated with the {@link TarArchiveEntry#TarArchiveEntry(byte[])} constructor.\n * These entries will be used when extracting from or listing the contents of an archive. These entries have their header filled in using the header bytes. They\n * also set the File to null, since they reference an archive entry not a file.\n * </p>\n * <p>\n * TarEntries that are created from Files that are to be written into an archive are instantiated with the {@link TarArchiveEntry#TarArchiveEntry(File)} or\n * {@link TarArchiveEntry#TarArchiveEntry(Path)} constructor. These entries have their header filled in using the File's information. They also keep a reference\n * to the File for convenience when writing entries.\n * </p>\n * <p>\n * Finally, TarEntries can be constructed from nothing but a name. This allows the programmer to construct the entry by hand, for instance when only an\n * InputStream is available for writing to the archive, and the header information is constructed from other information. In this case the header fields are set\n * to defaults and the File is set to null.\n * </p>\n * <p>\n * The C structure for a Tar Entry's header is:\n * </p>\n * <pre>\n * struct header {\n *   char name[100];     // TarConstants.NAMELEN    - offset   0\n *   char mode[8];       // TarConstants.MODELEN    - offset 100\n *   char uid[8];        // TarConstants.UIDLEN     - offset 108\n *   char gid[8];        // TarConstants.GIDLEN     - offset 116\n *   char size[12];      // TarConstants.SIZELEN    - offset 124\n *   char mtime[12];     // TarConstants.MODTIMELEN - offset 136\n *   char chksum[8];     // TarConstants.CHKSUMLEN  - offset 148\n *   char linkflag[1];   //                         - offset 156\n *   char linkname[100]; // TarConstants.NAMELEN    - offset 157\n *   // The following fields are only present in new-style POSIX tar archives:\n *   char magic[6];      // TarConstants.MAGICLEN   - offset 257\n *   char version[2];    // TarConstants.VERSIONLEN - offset 263\n *   char uname[32];     // TarConstants.UNAMELEN   - offset 265\n *   char gname[32];     // TarConstants.GNAMELEN   - offset 297\n *   char devmajor[8];   // TarConstants.DEVLEN     - offset 329\n *   char devminor[8];   // TarConstants.DEVLEN     - offset 337\n *   char prefix[155];   // TarConstants.PREFIXLEN  - offset 345\n *   // Used if ""name"" field is not long enough to hold the path\n *   char pad[12];       // NULs                    - offset 500\n * } header;\n * </pre>\n * <p>\n * All unused bytes are set to null. New-style GNU tar files are slightly different from the above. For values of size larger than 077777777777L (11 7s) or uid\n * and gid larger than 07777777L (7 7s) the sign bit of the first byte is set, and the rest of the field is the binary representation of the number. See\n * {@link TarUtils#parseOctalOrBinary(byte[], int, int)}.\n * <p>\n * The C structure for a old GNU Tar Entry's header is:\n * </p>\n * <pre>\n * struct oldgnu_header {\n *   char unused_pad1[345]; // TarConstants.PAD1LEN_GNU       - offset 0\n *   char atime[12];        // TarConstants.ATIMELEN_GNU      - offset 345\n *   char ctime[12];        // TarConstants.CTIMELEN_GNU      - offset 357\n *   char offset[12];       // TarConstants.OFFSETLEN_GNU     - offset 369\n *   char longnames[4];     // TarConstants.LONGNAMESLEN_GNU  - offset 381\n *   char unused_pad2;      // TarConstants.PAD2LEN_GNU       - offset 385\n *   struct sparse sp[4];   // TarConstants.SPARSELEN_GNU     - offset 386\n *   char isextended;       // TarConstants.ISEXTENDEDLEN_GNU - offset 482\n *   char realsize[12];     // TarConstants.REALSIZELEN_GNU   - offset 483\n *   char unused_pad[17];   // TarConstants.PAD3LEN_GNU       - offset 495\n * };\n * </pre>\n * <p>\n * Whereas, ""struct sparse"" is:\n * </p>\n * <pre>\n * struct sparse {\n *   char offset[12];   // offset 0\n *   char numbytes[12]; // offset 12\n * };\n * </pre>\n * <p>\n * The C structure for a xstar (Jörg Schilling star) Tar Entry's header is:\n * </p>\n * <pre>\n * struct star_header {\n *   char name[100];     // offset   0\n *   char mode[8];       // offset 100\n *   char uid[8];        // offset 108\n *   char gid[8];        // offset 116\n *   char size[12];      // offset 124\n *   char mtime[12];     // offset 136\n *   char chksum[8];     // offset 148\n *   char typeflag;      // offset 156\n *   char linkname[100]; // offset 157\n *   char magic[6];      // offset 257\n *   char version[2];    // offset 263\n *   char uname[32];     // offset 265\n *   char gname[32];     // offset 297\n *   char devmajor[8];   // offset 329\n *   char devminor[8];   // offset 337\n *   char prefix[131];   // offset 345\n *   char atime[12];     // offset 476\n *   char ctime[12];     // offset 488\n *   char mfill[8];      // offset 500\n *   char xmagic[4];     // offset 508  ""tar\0""\n * };\n * </pre>\n * <p>\n * which is identical to new-style POSIX up to the first 130 bytes of the prefix.\n * </p>\n * <p>\n * The C structure for the xstar-specific parts of a xstar Tar Entry's header is:\n * </p>\n * <pre>\n * struct xstar_in_header {\n *   char fill[345];         // offset 0     Everything before t_prefix\n *   char prefix[1];         // offset 345   Prefix for t_name\n *   char fill2;             // offset 346\n *   char fill3[8];          // offset 347\n *   char isextended;        // offset 355\n *   struct sparse sp[SIH];  // offset 356   8 x 12\n *   char realsize[12];      // offset 452   Real size for sparse data\n *   char offset[12];        // offset 464   Offset for multivolume data\n *   char atime[12];         // offset 476\n *   char ctime[12];         // offset 488\n *   char mfill[8];          // offset 500\n *   char xmagic[4];         // offset 508   ""tar\0""\n * };\n * </pre>\n *\n * @NotThreadSafe",public ,,org.apache.commons.compress.archivers.ArchiveEntry;org.apache.commons.compress.archivers.tar.TarConstants;org.apache.commons.compress.archivers.EntryStreamOffsets
org.apache.commons.compress.archivers.tar.TarUtils,Class,CheckStyle:HideUtilityClassConstructorCheck OFF (bc),public ,,
org.apache.commons.compress.archivers.ArchiveOutputStream,Abstract Class,"* Archive output stream implementations are expected to override the {@link #write(byte[], int, int)} method to improve performance. They should also override\n * {@link #close()} to ensure that any necessary trailers are added.\n *\n * <p>\n * The normal sequence of calls when working with ArchiveOutputStreams is:\n * </p>\n * <ul>\n * <li>Create ArchiveOutputStream object,</li>\n * <li>optionally write SFX header (Zip only),</li>\n * <li>repeat as needed:\n * <ul>\n * <li>{@link #putArchiveEntry(ArchiveEntry)} (writes entry header),\n * <li>{@link #write(byte[])} (writes entry data, as often as needed),\n * <li>{@link #closeArchiveEntry()} (closes entry),\n * </ul>\n * </li>\n * <li>{@link #finish()} (ends the addition of entries),</li>\n * <li>optionally write additional data, provided format supports it,</li>\n * <li>{@link #close()}.</li>\n * </ul>\n *\n * @param <E> The type of {@link ArchiveEntry} consumed.","public , abstract ",org.apache.commons.compress.CompressFilterOutputStream,
org.apache.commons.compress.archivers.ArchiveInputStream,Abstract Class,"* Archive input streams <strong>MUST</strong> override the {@link #read(byte[], int, int)} - or {@link #read()} - method so that reading from the stream\n * generates EOF for the end of data in each entry as well as at the end of the file proper.\n * <p>\n * The {@link #getNextEntry()} method is used to reset the input stream ready for reading the data from the next entry.\n * </p>\n * <p>\n * The input stream classes must also implement a method with the signature:\n * </p>\n *\n * <pre>\n * public static boolean matches(byte[] signature, int length)\n * </pre>\n * <p>\n * which is used by the {@link ArchiveStreamFactory} to autodetect the archive type from the first few bytes of a stream.\n * </p>\n *\n * @param <E> The type of {@link ArchiveEntry} produced.","public , abstract ",java.io.FilterInputStream,
org.apache.commons.compress.archivers.ArchiveInputStream$ArchiveEntryIOIterator,Class,* An iterator over a collection of a specific {@link ArchiveEntry} type.,final ,,org.apache.commons.io.function.IOIterator
org.apache.commons.compress.archivers.cpio.CpioConstants,Interface,"* All constants needed by CPIO.\n * <p>\n * Based on code from the <a href=""jrpm.sourceforge.net"">jRPM project</a>.\n * </p>\n * <p>\n * A list of the {@code C_xxx} constants is <a href=""http://www.opengroup.org/onlinepubs/9699919799/basedefs/cpio.h.html"">here</a>.\n * </p>\n * <p>\n * TODO Next major version: Update to a class.\n * </p>",public ,,
org.apache.commons.compress.archivers.cpio.CpioArchiveOutputStream,Class,"* CpioArchiveOutputStream is a stream for writing CPIO streams. All formats of CPIO are supported (old ASCII, old binary, new portable format and the new\n * portable format with CRC).\n *\n * <p>\n * An entry can be written by creating an instance of CpioArchiveEntry and fill it with the necessary values and put it into the CPIO stream. Afterwards write\n * the contents of the file into the CPIO stream. Either close the stream by calling finish() or put a next entry into the cpio stream.\n * </p>\n *\n * <pre>\n * CpioArchiveOutputStream out = new CpioArchiveOutputStream(\n *         new FileOutputStream(new File(""test.cpio"")));\n * CpioArchiveEntry entry = new CpioArchiveEntry();\n * entry.setName(""testfile"");\n * String contents = &quot;12345&quot;;\n * entry.setFileSize(contents.length());\n * entry.setMode(CpioConstants.C_ISREG); // regular file\n * ... set other attributes, for example time, number of links\n * out.putArchiveEntry(entry);\n * out.write(testContents.getBytes());\n * out.close();\n * </pre>\n *\n * <p>\n * Note: This implementation should be compatible to cpio 2.5\n * </p>\n *\n * <p>\n * This class uses mutable fields and is not considered threadsafe.\n * </p>\n *\n * <p>\n * based on code from the jRPM project (jrpm.sourceforge.net)\n * </p>",public ,org.apache.commons.compress.archivers.ArchiveOutputStream,org.apache.commons.compress.archivers.cpio.CpioConstants
org.apache.commons.compress.archivers.cpio.CpioArchiveInputStream,Class,"* CpioArchiveInputStream is a stream for reading cpio streams. All formats of cpio are supported (old ascii, old binary, new portable format and the new\n * portable format with CRC).\n * <p>\n * The stream can be read by extracting a cpio entry (containing all information about an entry) and afterwards reading from the stream the file specified by\n * the entry.\n * </p>\n * <pre>\n * CpioArchiveInputStream cpioIn = new CpioArchiveInputStream(Files.newInputStream(Paths.get(&quot;test.cpio&quot;)));\n * CpioArchiveEntry cpioEntry;\n *\n * while ((cpioEntry = cpioIn.getNextEntry()) != null) {\n *     System.out.println(cpioEntry.getName());\n *     int tmp;\n *     StringBuilder buf = new StringBuilder();\n *     while ((tmp = cpIn.read()) != -1) {\n *         buf.append((char) tmp);\n *     }\n *     System.out.println(buf.toString());\n * }\n * cpioIn.close();\n * </pre>\n * <p>\n * Note: This implementation should be compatible to cpio 2.5\n * </p>\n * <p>\n * This class uses mutable fields and is not considered to be threadsafe.\n * </p>\n * <p>\n * Based on code from the jRPM project (jrpm.sourceforge.net)\n * </p>",public ,org.apache.commons.compress.archivers.ArchiveInputStream,org.apache.commons.compress.archivers.cpio.CpioConstants
org.apache.commons.compress.archivers.cpio.CpioUtil,Class,* Package private utility class for Cpio\n *\n * @Immutable,final ,,
org.apache.commons.compress.archivers.cpio.CpioArchiveEntry,Class,"* A cpio archive consists of a sequence of files. There are several types of headers defined in two categories of new and old format. The headers are\n * recognized by magic numbers:\n *\n * <ul>\n * <li>""070701"" ASCII for new portable format</li>\n * <li>""070702"" ASCII for new portable format with CRC</li>\n * <li>""070707"" ASCII for old ASCII (also known as Portable ASCII, odc or old character format</li>\n * <li>070707 binary for old binary</li>\n * </ul>\n *\n * <p>\n * The old binary format is limited to 16 bits for user id, group id, device, and inode numbers. It is limited to 4 gigabyte file sizes.\n *\n * The old ASCII format is limited to 18 bits for the user id, group id, device, and inode numbers. It is limited to 8 gigabyte file sizes.\n *\n * The new ASCII format is limited to 4 gigabyte file sizes.\n *\n * CPIO 2.5 knows also about tar, but it is not recognized here.\n * </p>\n *\n *\n * <h2>OLD FORMAT</h2>\n *\n * <p>\n * Each file has a 76 (ascii) / 26 (binary) byte header, a variable length, NUL terminated file name, and variable length file data. A header for a file name\n * ""TRAILER!!!"" indicates the end of the archive.\n * </p>\n *\n * <p>\n * All the fields in the header are ISO 646 (approximately ASCII) strings of octal numbers, left padded, not NUL terminated.\n * </p>\n *\n * <pre>\n * FIELDNAME        NOTES\n * c_magic          The integer value octal 070707.  This value can be used to deter-\n *                  mine whether this archive is written with little-endian or big-\n *                  endian integers.\n * c_dev            Device that contains a directory entry for this file\n * c_ino            I-node number that identifies the input file to the file system\n * c_mode           The mode specifies both the regular permissions and the file type.\n * c_uid            Numeric User ID of the owner of the input file\n * c_gid            Numeric Group ID of the owner of the input file\n * c_nlink          Number of links that are connected to the input file\n * c_rdev           For block special and character special entries, this field\n *                  contains the associated device number.  For all other entry types,\n *                  it should be set to zero by writers and ignored by readers.\n * c_mtime[2]       Modification time of the file, indicated as the number of seconds\n *                  since the start of the epoch, 00:00:00 UTC January 1, 1970.  The\n *                  four-byte integer is stored with the most-significant 16 bits\n *                  first followed by the least-significant 16 bits.  Each of the two\n *                  16 bit values are stored in machine-native byte order.\n * c_namesize       Length of the path name, including the terminating null byte\n * c_filesize[2]    Length of the file in bytes. This is the length of the data\n *                  section that follows the header structure. Must be 0 for\n *                  FIFOs and directories\n *\n * All fields are unsigned short fields with 16-bit integer values\n * apart from c_mtime and c_filesize which are 32-bit integer values\n * </pre>\n *\n * <p>\n * If necessary, the file name and file data are padded with a NUL byte to an even length\n * </p>\n *\n * <p>\n * Special files, directories, and the trailer are recorded with the h_filesize field equal to 0.\n * </p>\n *\n * <p>\n * In the ASCII version of this format, the 16-bit entries are represented as 6-byte octal numbers, and the 32-bit entries are represented as 11-byte octal\n * numbers. No padding is added.\n * </p>\n *\n * <h3>NEW FORMAT</h3>\n *\n * <p>\n * Each file has a 110 byte header, a variable length, NUL terminated file name, and variable length file data. A header for a file name ""TRAILER!!!"" indicates\n * the end of the archive. All the fields in the header are ISO 646 (approximately ASCII) strings of hexadecimal numbers, left padded, not NUL terminated.\n * </p>\n *\n * <pre>\n * FIELDNAME        NOTES\n * c_magic[6]       The string 070701 for new ASCII, the string 070702 for new ASCII with CRC\n * c_ino[8]\n * c_mode[8]\n * c_uid[8]\n * c_gid[8]\n * c_nlink[8]\n * c_mtim[8]\n * c_filesize[8]    must be 0 for FIFOs and directories\n * c_maj[8]\n * c_min[8]\n * c_rmaj[8]        only valid for chr and blk special files\n * c_rmin[8]        only valid for chr and blk special files\n * c_namesize[8]    count includes terminating NUL in path name\n * c_check[8]       0 for ""new"" portable format; for CRC format\n *                  the sum of all the bytes in the file\n * </pre>\n *\n * <p>\n * New ASCII Format The ""new"" ASCII format uses 8-byte hexadecimal fields for all numbers and separates device numbers into separate fields for major and minor\n * numbers.\n * </p>\n *\n * <p>\n * The path name is followed by NUL bytes so that the total size of the fixed header plus path name is a multiple of four. Likewise, the file data is padded to\n * a multiple of four bytes.\n * </p>\n *\n * <p>\n * This class uses mutable fields and is not considered to be threadsafe.\n * </p>\n *\n * <p>\n * Based on code from the jRPM project (https://jrpm.sourceforge.net).\n * </p>\n *\n * <p>\n * The MAGIC numbers and other constants are defined in {@link CpioConstants}\n * </p>\n *\n * <p>\n * N.B. does not handle the cpio ""tar"" format\n * </p>\n *\n * @NotThreadSafe\n * @see <a href=""https://people.freebsd.org/~kientzle/libarchive/man/cpio.5.txt"">https://people.freebsd.org/~kientzle/libarchive/man/cpio.5.txt</a>",public ,,org.apache.commons.compress.archivers.cpio.CpioConstants;org.apache.commons.compress.archivers.ArchiveEntry
org.apache.commons.compress.java.util.jar.Pack200,Abstract Class,* Class factory for {@link Pack200.Packer} and {@link Pack200.Unpacker}.,"public , abstract ",,
org.apache.commons.compress.java.util.jar.Pack200$Packer,Interface,* The interface defining the API for converting a JAR file to an output stream in the Pack200 format.,public ,,
org.apache.commons.compress.java.util.jar.Pack200$Unpacker,Interface,* The interface defining the API for converting a packed stream in the Pack200 format to a JAR file.,public ,,
org.apache.commons.compress.utils.CRC32VerifyingInputStream,Class,* A stream that verifies the CRC of the data read once the stream is exhausted.\n *\n * @NotThreadSafe\n * @since 1.6\n * @deprecated Use {@link org.apache.commons.io.input.ChecksumInputStream}.,public ,org.apache.commons.compress.utils.ChecksumVerifyingInputStream,
org.apache.commons.compress.utils.IOUtils,Class,* Utility functions.\n *\n * @Immutable (has mutable data but it is write-only).,"public , final ",,
org.apache.commons.compress.utils.Sets,Class,* Set utilities\n *\n * @since 1.13,public ,,
org.apache.commons.compress.utils.ByteUtils,Class,* Utility methods for reading and writing bytes.\n *\n * @since 1.14,"public , final ",,
org.apache.commons.compress.utils.ByteUtils$ByteConsumer,Interface,* Used to consume bytes.\n     *\n     * @since 1.14,public ,,
org.apache.commons.compress.utils.ByteUtils$ByteSupplier,Interface,* Used to supply bytes.\n     *\n     * @since 1.14,public ,,
org.apache.commons.compress.utils.ByteUtils$InputStreamByteSupplier,Class,* {@link ByteSupplier} based on {@link InputStream}.\n     *\n     * @since 1.14\n     * @deprecated Unused,"public , static ",,org.apache.commons.compress.utils.ByteUtils.ByteSupplier
org.apache.commons.compress.utils.ByteUtils$OutputStreamByteConsumer,Class,* {@link ByteConsumer} based on {@link OutputStream}.\n     *\n     * @since 1.14,"public , static ",,org.apache.commons.compress.utils.ByteUtils.ByteConsumer
org.apache.commons.compress.utils.Iterators,Class,* Iterator utilities.\n *\n * @since 1.13.,public ,,
org.apache.commons.compress.utils.ArchiveUtils,Class,* Generic Archive utilities,public ,,
org.apache.commons.compress.utils.OsgiUtils,Class,* Utilities for dealing with OSGi environments.\n *\n * @since 1.21,public ,,
org.apache.commons.compress.utils.Charsets,Class,"* Charsets required of every implementation of the Java platform.\n *\n * From the Java documentation <a href=""https://docs.oracle.com/javase/8/docs/api/java/nio/charset/Charset.html"">Standard charsets</a>:\n * <p>\n * <cite>Every implementation of the Java platform is required to support the following character encodings. Consult the release documentation for your\n * implementation to see if any other encodings are supported. Consult the release documentation for your implementation to see if any other encodings are\n * supported. </cite>\n * </p>\n *\n * <dl>\n * <dt>{@code US-ASCII}</dt>\n * <dd>Seven-bit ASCII, a.k.a. ISO646-US, a.k.a. the Basic Latin block of the Unicode character set.</dd>\n * <dt>{@code ISO-8859-1}</dt>\n * <dd>ISO Latin Alphabet No. 1, a.k.a. ISO-LATIN-1.</dd>\n * <dt>{@code UTF-8}</dt>\n * <dd>Eight-bit Unicode Transformation Format.</dd>\n * <dt>{@code UTF-16BE}</dt>\n * <dd>Sixteen-bit Unicode Transformation Format, big-endian byte order.</dd>\n * <dt>{@code UTF-16LE}</dt>\n * <dd>Sixteen-bit Unicode Transformation Format, little-endian byte order.</dd>\n * <dt>{@code UTF-16}</dt>\n * <dd>Sixteen-bit Unicode Transformation Format, byte order specified by a mandatory initial byte-order mark (either order accepted on input, big-endian used\n * on output.)</dd>\n * </dl>\n *\n * <p>\n * This class best belongs in the Commons Lang or IO project. Even if a similar class is defined in another Commons component, it is not foreseen that Commons\n * Compress would be made to depend on another Commons component.\n * </p>\n *\n * @see <a href=""https://docs.oracle.com/javase/8/docs/api/java/nio/charset/Charset.html"">Standard charsets</a>\n * @see StandardCharsets\n * @since 1.4\n * @deprecated Use {@link org.apache.commons.io.Charsets}.",public ,,
org.apache.commons.compress.utils.ExactMath,Class,"* PRIVATE.\n *\n * Performs exact math through {@link Math} ""exact"" APIs.",public ,,
org.apache.commons.compress.utils.BoundedArchiveInputStream,Abstract Class,* NIO backed bounded input stream for reading a predefined amount of data from.\n *\n * @ThreadSafe this base class is thread safe but implementations must not be.\n * @since 1.21,"public , abstract ",java.io.InputStream,
org.apache.commons.compress.utils.Lists,Class,* List utilities\n *\n * @since 1.13,public ,,
org.apache.commons.compress.utils.ChecksumVerifyingInputStream,Class,* Verifies the checksum of the data read once the stream is exhausted.\n *\n * @NotThreadSafe\n * @since 1.7\n * @deprecated Use {@link org.apache.commons.io.input.ChecksumInputStream}.,public ,java.util.zip.CheckedInputStream,
org.apache.commons.compress.utils.SkipShieldingInputStream,Class,* A wrapper that overwrites {@link #skip} and delegates to {@link #read} instead.\n *\n * <p>\n * Some implementations of {@link InputStream} implement {@link InputStream#skip} in a way that throws an exception if the stream is not seekable -\n * {@link System#in System.in} is known to behave that way. For such a stream it is impossible to invoke skip at all and you have to read from the stream (and\n * discard the data read) instead. Skipping is potentially much faster than reading so we do want to invoke {@code skip} when possible. We provide this class so\n * you can wrap your own {@link InputStream} in it if you encounter problems with {@code skip} throwing an exception.\n * </p>\n *\n * @since 1.17\n * @deprecated No longer used.,public ,java.io.FilterInputStream,
org.apache.commons.compress.utils.ChecksumCalculatingInputStream,Class,* Calculates the checksum of the data read.\n *\n * @NotThreadSafe\n * @since 1.14\n * @deprecated Use {@link CheckedInputStream}.,public ,java.util.zip.CheckedInputStream,
org.apache.commons.compress.utils.BoundedInputStream,Class,* A stream that limits reading from a wrapped stream to a given number of bytes.\n *\n * @NotThreadSafe\n * @since 1.6\n * @deprecated Use {@link org.apache.commons.io.input.BoundedInputStream}.,public ,org.apache.commons.io.input.BoundedInputStream,
org.apache.commons.compress.utils.FlushShieldFilterOutputStream,Class,* Re-implements {@link FilterOutputStream#flush()} to do nothing.,public ,java.io.FilterOutputStream,
org.apache.commons.compress.utils.FixedLengthBlockOutputStream,Class,"* This class supports writing to an OutputStream or WritableByteChannel in fixed length blocks.\n * <p>\n * It can be be used to support output to devices such as tape drives that require output in this format. If the final block does not have enough content to\n * fill an entire block, the output will be padded to a full block size.\n * </p>\n *\n * <p>\n * This class can be used to support TAR,PAX, and CPIO blocked output to character special devices. It is not recommended that this class be used unless writing\n * to such devices, as the padding serves no useful purpose in such cases.\n * </p>\n *\n * <p>\n * This class should normally wrap a FileOutputStream or associated WritableByteChannel directly. If there is an intervening filter that modified the output,\n * such as a CompressorOutputStream, or performs its own buffering, such as BufferedOutputStream, output to the device may no longer be of the specified size.\n * </p>\n *\n * <p>\n * Any content written to this stream should be self-delimiting and should tolerate any padding added to fill the last block.\n * </p>\n *\n * @since 1.15",public ,java.io.OutputStream,java.nio.channels.WritableByteChannel
org.apache.commons.compress.utils.FixedLengthBlockOutputStream$BufferAtATimeOutputChannel,Class,"* Helper class to provide channel wrapper for arbitrary output stream that doesn't alter the size of writes. We can't use Channels.newChannel, because for\n     * non FileOutputStreams, it breaks up writes into 8KB max chunks. Since the purpose of this class is to always write complete blocks, we need to write a\n     * simple class to take care of it.","private , static , final ",,java.nio.channels.WritableByteChannel
org.apache.commons.compress.utils.BitInputStream,Class,* Reads bits from an InputStream.\n *\n * @since 1.10\n * @NotThreadSafe,public ,,java.io.Closeable
org.apache.commons.compress.utils.TimeUtils,Class,"* Utility class for handling time-related types and conversions.\n * <p>\n * Understanding Unix vs NTFS timestamps:\n * </p>\n * <ul>\n * <li>A <a href=""https://en.wikipedia.org/wiki/Unix_time"">Unix timestamp</a> is a primitive long starting at the Unix Epoch on January 1st, 1970 at Coordinated\n * Universal Time (UTC)</li>\n * <li>An <a href=""https://learn.microsoft.com/en-us/windows/win32/sysinfo/file-times"">NTFS timestamp</a> is a file time is a 64-bit value that represents the\n * number of 100-nanosecond intervals that have elapsed since 12:00 A.M. January 1, 1601 Coordinated Universal Time (UTC).</li>\n * </ul>\n *\n * @since 1.23","public , final ",,
org.apache.commons.compress.utils.CountingOutputStream,Class,* Stream that tracks the number of bytes read.\n *\n * @since 1.3\n * @NotThreadSafe\n * @deprecated Use {@link org.apache.commons.io.output.CountingOutputStream}.,public ,java.io.FilterOutputStream,
org.apache.commons.compress.utils.ParsingUtils,Class,* Utility methods for parsing data and converting it to other formats.\n *\n * @since 1.26.0,"public , final ",,
org.apache.commons.compress.utils.CountingInputStream,Class,* Input stream that tracks the number of bytes read.\n *\n * @since 1.3\n * @NotThreadSafe\n * @deprecated Use {@link org.apache.commons.io.input.CountingInputStream}.,public ,java.io.FilterInputStream,
org.apache.commons.compress.utils.SeekableInMemoryByteChannel,Class,* A {@link SeekableByteChannel} implementation that wraps a byte[].\n * <p>\n * When this channel is used for writing an internal buffer grows to accommodate incoming data. The natural size limit is the value of {@link Integer#MAX_VALUE}\n * and it is not possible to {@link #position(long) set the position} or {@link #truncate truncate} to a value bigger than that. Internal buffer can be accessed\n * via {@link SeekableInMemoryByteChannel#array()}.\n * </p>\n *\n * @since 1.13\n * @NotThreadSafe,public ,,java.nio.channels.SeekableByteChannel
org.apache.commons.compress.utils.MultiReadOnlySeekableByteChannel,Class,"* Implements a read-only {@link SeekableByteChannel} that concatenates a collection of other {@link SeekableByteChannel}s.\n * <p>\n * This is a lose port of <a href=\n * ""https://github.com/frugalmechanic/fm-common/blob/master/jvm/src/main/scala/fm/common/MultiReadOnlySeekableByteChannel.scala"">\n * MultiReadOnlySeekableByteChannel</a>\n * by Tim Underwood.\n * </p>\n *\n * @since 1.19",public ,,java.nio.channels.SeekableByteChannel
org.apache.commons.compress.utils.FileNameUtils,Class,* Generic file name utilities.\n *\n * @since 1.20,public ,,
org.apache.commons.compress.utils.CloseShieldFilterInputStream,Class,* Re-implements {@link FilterInputStream#close()} to do nothing.\n *\n * @since 1.14\n * @deprecated Use {@link org.apache.commons.io.input.CloseShieldInputStream}.,public ,java.io.FilterInputStream,
org.apache.commons.compress.utils.BoundedSeekableByteChannelInputStream,Class,"* InputStream that delegates requests to the underlying SeekableByteChannel, making sure that only bytes from a certain range can be read.\n *\n * @ThreadSafe\n * @since 1.21",public ,org.apache.commons.compress.utils.BoundedArchiveInputStream,
org.apache.commons.compress.utils.ServiceLoaderIterator,Class,* Iterates all services for a given class through the standard {@link ServiceLoader} mechanism.\n *\n * @param <E> The service to load\n * @since 1.13\n * @deprecated No longer needed.,public ,,java.util.Iterator
org.apache.commons.compress.utils.CharsetNames,Class,"* Character encoding names required of every implementation of the Java platform.\n *\n * From the Java documentation <a href=""https://docs.oracle.com/javase/8/docs/api/java/nio/charset/Charset.html"">Standard charsets</a>:\n * <p>\n * <cite>Every implementation of the Java platform is required to support the following character encodings. Consult the release documentation for your\n * implementation to see if any other encodings are supported. Consult the release documentation for your implementation to see if any other encodings are\n * supported. </cite>\n * </p>\n *\n * <dl>\n * <dt>{@code US-ASCII}</dt>\n * <dd>Seven-bit ASCII, a.k.a. ISO646-US, a.k.a. the Basic Latin block of the Unicode character set.</dd>\n * <dt>{@code ISO-8859-1}</dt>\n * <dd>ISO Latin Alphabet No. 1, a.k.a. ISO-LATIN-1.</dd>\n * <dt>{@code UTF-8}</dt>\n * <dd>Eight-bit Unicode Transformation Format.</dd>\n * <dt>{@code UTF-16BE}</dt>\n * <dd>Sixteen-bit Unicode Transformation Format, big-endian byte order.</dd>\n * <dt>{@code UTF-16LE}</dt>\n * <dd>Sixteen-bit Unicode Transformation Format, little-endian byte order.</dd>\n * <dt>{@code UTF-16}</dt>\n * <dd>Sixteen-bit Unicode Transformation Format, byte order specified by a mandatory initial byte-order mark (either order accepted on input, big-endian used\n * on output.)</dd>\n * </dl>\n *\n * <p>\n * This perhaps would best belong in the [lang] project. Even if a similar interface is defined in [lang], it is not foreseen that [compress] would be made to\n * depend on [lang].\n * </p>\n *\n * @see <a href=""https://docs.oracle.com/javase/8/docs/api/java/nio/charset/Charset.html"">Standard charsets</a>\n * @since 1.4\n * @deprecated Use {@link StandardCharsets}.",public ,,
org.apache.commons.compress.utils.InputStreamStatistics,Interface,"* This interface provides statistics on the current decompression stream. The stream consumer can use that statistics to handle abnormal compression ratios,\n * i.e. to prevent ZIP bombs.\n *\n * @since 1.17",public ,,
org.apache.commons.compress.CompressFilterOutputStream,Abstract Class,* Abstracts classes that compress or archive an output stream.\n *\n * @param <T> The underlying {@link OutputStream} type.\n * @since 1.28.0,"public , abstract ",java.io.FilterOutputStream,
org.apache.commons.compress.compressors.CompressorOutputStream,Abstract Class,* Abstracts all classes that compress an output stream.\n *\n * @param <T> The underlying {@link OutputStream} type.,"public , abstract ",org.apache.commons.compress.CompressFilterOutputStream,
org.apache.commons.compress.compressors.deflate64.Deflate64CompressorInputStream,Class,* Deflate64 decompressor.\n *\n * @since 1.16\n * @NotThreadSafe,public ,org.apache.commons.compress.compressors.CompressorInputStream,org.apache.commons.compress.utils.InputStreamStatistics
org.apache.commons.compress.compressors.deflate64.HuffmanDecoder,Class,* TODO This class can't be final because it is mocked by Mockito.,default,,java.io.Closeable
org.apache.commons.compress.compressors.deflate64.HuffmanDecoder$BinaryTreeNode,Class,No Comment,"private , static , final ",,
org.apache.commons.compress.compressors.deflate64.HuffmanDecoder$DecodingMemory,Class,No Comment,"private , static , final ",,
org.apache.commons.compress.compressors.deflate64.HuffmanDecoder$HuffmanCodes,Class,No Comment,"private , final ",org.apache.commons.compress.compressors.deflate64.HuffmanDecoder.DecoderState,
org.apache.commons.compress.compressors.deflate64.HuffmanDecoder$InitialState,Class,No Comment,"private , static , final ",org.apache.commons.compress.compressors.deflate64.HuffmanDecoder.DecoderState,
org.apache.commons.compress.compressors.deflate64.HuffmanDecoder$UncompressedState,Class,No Comment,"private , final ",org.apache.commons.compress.compressors.deflate64.HuffmanDecoder.DecoderState,
org.apache.commons.compress.compressors.deflate64.HuffmanDecoder$DecoderState,Abstract Class,No Comment,"private , abstract , static ",,
org.apache.commons.compress.compressors.deflate64.HuffmanState,Enum,No Comment,default,enum has no extends,
org.apache.commons.compress.compressors.brotli.BrotliCompressorInputStream,Class,"* {@link CompressorInputStream} implementation to decode Brotli encoded stream. Library relies on <a href=""https://github.com/google/brotli"">Google brotli</a>\n *\n * @since 1.14",public ,org.apache.commons.compress.compressors.CompressorInputStream,org.apache.commons.compress.utils.InputStreamStatistics
org.apache.commons.compress.compressors.brotli.BrotliUtils,Class,* Utility code for the Brotli compression format.\n *\n * @ThreadSafe\n * @since 1.14,public ,,
org.apache.commons.compress.compressors.brotli.BrotliUtils$CachedAvailability,Enum,No Comment,,enum has no extends,
org.apache.commons.compress.compressors.lz4.BlockLZ4CompressorInputStream,Class,"* CompressorInputStream for the LZ4 block format.\n *\n * @see <a href=""https://lz4.github.io/lz4/lz4_Block_format.html"">LZ4 Block Format Description</a>\n * @since 1.14\n * @NotThreadSafe",public ,org.apache.commons.compress.compressors.lz77support.AbstractLZ77CompressorInputStream,
org.apache.commons.compress.compressors.lz4.BlockLZ4CompressorInputStream$State,Enum,No Comment,private ,enum has no extends,
org.apache.commons.compress.compressors.lz4.FramedLZ4CompressorOutputStream,Class,"* CompressorOutputStream for the LZ4 frame format.\n *\n * <p>\n * Based on the ""spec"" in the version ""1.5.1 (31/03/2015)""\n * </p>\n *\n * @see <a href=""https://lz4.github.io/lz4/lz4_Frame_format.html"">LZ4 Frame Format Description</a>\n * @since 1.14\n * @NotThreadSafe",public ,org.apache.commons.compress.compressors.CompressorOutputStream,
org.apache.commons.compress.compressors.lz4.FramedLZ4CompressorOutputStream$BlockSize,Enum,* Enumerates the block sizes supported by the format.,public ,enum has no extends,
org.apache.commons.compress.compressors.lz4.FramedLZ4CompressorOutputStream$Parameters,Class,* Parameters of the LZ4 frame format.,"public , static ",,
org.apache.commons.compress.compressors.lz4.FramedLZ4CompressorInputStream,Class,"* CompressorInputStream for the LZ4 frame format.\n *\n * <p>\n * Based on the ""spec"" in the version ""1.5.1 (31/03/2015)""\n * </p>\n *\n * @see <a href=""https://lz4.github.io/lz4/lz4_Frame_format.html"">LZ4 Frame Format Description</a>\n * @since 1.14\n * @NotThreadSafe",public ,org.apache.commons.compress.compressors.CompressorInputStream,org.apache.commons.compress.utils.InputStreamStatistics
org.apache.commons.compress.compressors.lz4.BlockLZ4CompressorOutputStream,Class,"* CompressorOutputStream for the LZ4 block format.\n *\n * @see <a href=""https://lz4.github.io/lz4/lz4_Block_format.html"">LZ4 Block Format Description</a>\n * @since 1.14\n * @NotThreadSafe",public ,org.apache.commons.compress.compressors.CompressorOutputStream,
org.apache.commons.compress.compressors.lz4.BlockLZ4CompressorOutputStream$Pair,Class,No Comment,"static , final ",,
org.apache.commons.compress.compressors.lz4.XXHash32,Class,"* Implements the xxhash32 hash algorithm.\n *\n * @see <a href=""https://cyan4973.github.io/xxHash/"">xxHash</a>\n * @NotThreadSafe\n * @since 1.14\n * @deprecated Use {@link org.apache.commons.codec.digest.XXHash32}.",public ,org.apache.commons.codec.digest.XXHash32,
org.apache.commons.compress.compressors.gzip.GzipParameters,Class,"* Parameters for the GZIP compressor.\n *\n * @see GzipCompressorInputStream\n * @see GzipCompressorOutputStream\n * @see <a href=""https://datatracker.ietf.org/doc/html/rfc1952"">RFC 1952 GZIP File Format Specification</a>\n * @since 1.7",public ,,
org.apache.commons.compress.compressors.gzip.GzipParameters$OS,Enum,"* Enumerates OS types.\n     * <ul>\n     * <li>0 - FAT filesystem (MS-DOS, OS/2, NT/Win32)</li>\n     * <li>1 - Amiga</li>\n     * <li>2 - VMS (or OpenVMS)</li>\n     * <li>3 - Unix</li>\n     * <li>4 - VM/CMS</li>\n     * <li>5 - Atari TOS</li>\n     * <li>6 - HPFS filesystem (OS/2, NT)</li>\n     * <li>7 - Macintosh</li>\n     * <li>8 - Z-System</li>\n     * <li>9 - CP/M</li>\n     * <li>10 - TOPS-20</li>\n     * <li>11 - NTFS filesystem (NT)</li>\n     * <li>12 - QDOS</li>\n     * <li>13 - Acorn RISCOS</li>\n     * <li>255 - unknown</li>\n     * </ul>\n     *\n     * @see <a href=""https://datatracker.ietf.org/doc/html/rfc1952#page-7"">RFC 1952: GZIP File Format Specification - OS (Operating System)</a>\n     * @since 1.28.0",public ,enum has no extends,
org.apache.commons.compress.compressors.gzip.GzipUtils,Class,"* Utility code for the GZIP compression format.\n *\n * @see <a href=""https://datatracker.ietf.org/doc/html/rfc1952"">RFC 1952 GZIP File Format Specification</a>\n * @ThreadSafe",public ,,
org.apache.commons.compress.compressors.gzip.GzipCompressorOutputStream,Class,"* Compressed output stream using the gzip format. This implementation improves over the standard {@link GZIPOutputStream} class by allowing the configuration\n * of the compression level and the header metadata (file name, comment, modification time, operating system and extra flags).\n *\n * @see <a href=""https://datatracker.ietf.org/doc/html/rfc1952"">RFC 1952 GZIP File Format Specification</a>",public ,org.apache.commons.compress.compressors.CompressorOutputStream,
org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream,Class,"* Input stream that decompresses GZIP (.gz) files.\n *\n * <p>\n * This supports decompressing concatenated GZIP files which is important when decompressing standalone GZIP files.\n * </p>\n * <p>\n * Instead of using {@code java.util.zip.GZIPInputStream}, this class has its own GZIP member decoder. Internally, decompression is done using\n * {@link java.util.zip.Inflater}.\n * </p>\n * <p>\n * If you use the constructor {@code GzipCompressorInputStream(in)}, {@code Builder.setDecompressConcatenated(false)}, or\n * {@code GzipCompressorInputStream(in, false)}, then {@link #read} will return -1 as soon as the first encoded GZIP member has been completely read. In this\n * case, if the underlying input stream supports {@link InputStream#mark mark()} and {@link InputStream#reset reset()}, then it will be left positioned just\n * after the end of the encoded GZIP member; otherwise, some indeterminate number of extra bytes following the encoded GZIP member will have been consumed and\n * discarded.\n * </p>\n * <p>\n * If you use the {@code Builder.setDecompressConcatenated(true)} or {@code GzipCompressorInputStream(in, true)} then {@link #read} will return -1 only after\n * the entire input stream has been exhausted; any bytes that follow an encoded GZIP member must constitute a new encoded GZIP member, otherwise an\n * {@link IOException} is thrown. The data read from a stream constructed this way will consist of the concatenated data of all of the encoded GZIP members in\n * order.\n * </p>\n * <p>\n * To build an instance, use {@link Builder}.\n * </p>\n *\n * @see Builder\n * @see <a href=""https://datatracker.ietf.org/doc/html/rfc1952"">RFC 1952 GZIP File Format Specification</a>",public ,org.apache.commons.compress.compressors.CompressorInputStream,org.apache.commons.compress.utils.InputStreamStatistics
org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream$Builder,Class,@formatter:on,"public , static ",org.apache.commons.io.build.AbstractStreamBuilder,
org.apache.commons.compress.compressors.gzip.ExtraField,Class,"* If the {@code FLG.FEXTRA} bit is set, an ""extra field"" is present in the header, with total length XLEN bytes.\n *\n * <pre>\n * +---+---+=================================+\n * | XLEN  |...XLEN bytes of ""extra field""...| (more...)\n * +---+---+=================================+\n * </pre>\n *\n * This class represents the extra field payload (excluding the XLEN 2 bytes). The ExtraField payload consists of a series of subfields, each of the form:\n *\n * <pre>\n * +---+---+---+---+==================================+\n * |SI1|SI2|  LEN  |... LEN bytes of subfield data ...|\n * +---+---+---+---+==================================+\n * </pre>\n *\n * This class does not expose the internal subfields list to prevent adding subfields without total extra length validation. The class is iterable, but this\n * iterator is immutable.\n *\n * @see <a href=""https://datatracker.ietf.org/doc/html/rfc1952"">RFC 1952 GZIP File Format Specification</a>\n * @since 1.28.0","public , final ",,java.lang.Iterable
org.apache.commons.compress.compressors.gzip.ExtraField$SubField,Class,"* If the {@code FLG.FEXTRA} bit is set, an ""extra field"" is present in the header, with total length XLEN bytes. It consists of a series of subfields, each\n     * of the form:\n     *\n     * <pre>\n     * +---+---+---+---+==================================+\n     * |SI1|SI2|  LEN  |... LEN bytes of subfield data ...|\n     * +---+---+---+---+==================================+\n     * </pre>\n     * <p>\n     * The reserved IDs are:\n     * </p>\n     *\n     * <pre>\n     * SI1         SI2         Data\n     * ----------  ----------  ----\n     * 0x41 ('A')  0x70 ('P')  Apollo file type information\n     * </pre>\n     * <p>\n     * Subfield IDs with {@code SI2 = 0} are reserved for future use.\n     * </p>\n     * <p>\n     * LEN gives the length of the subfield data, excluding the 4 initial bytes.\n     * </p>\n     *\n     * @see <a href=""https://datatracker.ietf.org/doc/html/rfc1952"">RFC 1952 GZIP File Format Specification</a>","public , static , final ",,
org.apache.commons.compress.compressors.lzw.LZWInputStream,Abstract Class,"* <p>\n * Generic LZW implementation. It is used internally for the Z decompressor and the Unshrinking Zip file compression method, but may be useful for third-party\n * projects in implementing their own LZW variations.\n * </p>\n *\n * @NotThreadSafe\n * @since 1.10","public , abstract ",org.apache.commons.compress.compressors.CompressorInputStream,org.apache.commons.compress.utils.InputStreamStatistics
org.apache.commons.compress.compressors.z.ZCompressorInputStream,Class,* Input stream that decompresses .Z files.\n *\n * @NotThreadSafe\n * @since 1.7,public ,org.apache.commons.compress.compressors.lzw.LZWInputStream,
org.apache.commons.compress.compressors.xz.XZCompressorOutputStream,Class,* Compresses an output stream using the XZ and LZMA2 compression options.\n *\n * <em>Calling flush()</em>\n * <p>\n * Calling {@link #flush()} flushes the encoder and calls {@code outputStream.flush()}. All buffered pending data will then be decompressible from the output\n * stream. Calling this function very often may increase the compressed file size a lot.\n * </p>\n *\n * @since 1.4,public ,org.apache.commons.compress.compressors.CompressorOutputStream,
org.apache.commons.compress.compressors.xz.XZCompressorInputStream,Class,* XZ decompressor.\n *\n * @since 1.4,public ,org.apache.commons.compress.compressors.CompressorInputStream,org.apache.commons.compress.utils.InputStreamStatistics
org.apache.commons.compress.compressors.xz.XZUtils,Class,* Utility code for the XZ compression format.\n *\n * @ThreadSafe\n * @since 1.4,public ,,
org.apache.commons.compress.compressors.xz.XZUtils$CachedAvailability,Enum,No Comment,,enum has no extends,
org.apache.commons.compress.compressors.CompressorInputStream,Abstract Class,* Abstracts services for all compressor input streams.,"public , abstract ",java.io.InputStream,
org.apache.commons.compress.compressors.snappy.FramedSnappyCompressorOutputStream,Class,"* CompressorOutputStream for the framing Snappy format.\n *\n * <p>\n * Based on the ""spec"" in the version ""Last revised: 2013-10-25""\n * </p>\n *\n * @see <a href=""https://github.com/google/snappy/blob/master/framing_format.txt"">Snappy framing format description</a>\n * @since 1.14\n * @NotThreadSafe",public ,org.apache.commons.compress.compressors.CompressorOutputStream,
org.apache.commons.compress.compressors.snappy.FramedSnappyDialect,Enum,* Enumerates dialects of the framing format that {@link FramedSnappyCompressorInputStream} can deal with.\n *\n * @since 1.12,public ,enum has no extends,
org.apache.commons.compress.compressors.snappy.SnappyCompressorInputStream,Class,"* CompressorInputStream for the raw Snappy format.\n *\n * <p>\n * This implementation uses an internal buffer in order to handle the back-references that are at the heart of the LZ77 algorithm. The size of the buffer must\n * be at least as big as the biggest offset used in the compressed stream. The current version of the Snappy algorithm as defined by Google works on 32k blocks\n * and doesn't contain offsets bigger than 32k which is the default block size used by this class.\n * </p>\n *\n * @see <a href=""https://github.com/google/snappy/blob/master/format_description.txt"">Snappy compressed format description</a>\n * @since 1.7",public ,org.apache.commons.compress.compressors.lz77support.AbstractLZ77CompressorInputStream,
org.apache.commons.compress.compressors.snappy.SnappyCompressorInputStream$State,Enum,No Comment,private ,enum has no extends,
org.apache.commons.compress.compressors.snappy.FramedSnappyCompressorInputStream,Class,"* CompressorInputStream for the framing Snappy format.\n *\n * <p>\n * Based on the ""spec"" in the version ""Last revised: 2013-10-25""\n * </p>\n *\n * @see <a href=""https://github.com/google/snappy/blob/master/framing_format.txt"">Snappy framing format description</a>\n * @since 1.7",public ,org.apache.commons.compress.compressors.CompressorInputStream,org.apache.commons.compress.utils.InputStreamStatistics
org.apache.commons.compress.compressors.snappy.SnappyCompressorOutputStream,Class,"* CompressorOutputStream for the raw Snappy format.\n *\n * <p>\n * This implementation uses an internal buffer in order to handle the back-references that are at the heart of the LZ77 algorithm. The size of the buffer must\n * be at least as big as the biggest offset used in the compressed stream. The current version of the Snappy algorithm as defined by Google works on 32k blocks\n * and doesn't contain offsets bigger than 32k which is the default block size used by this class.\n * </p>\n *\n * <p>\n * The raw Snappy format requires the uncompressed size to be written at the beginning of the stream using a varint representation, i.e. the number of bytes\n * needed to write the information is not known before the uncompressed size is known. We've chosen to make the uncompressedSize a parameter of the constructor\n * in favor of buffering the whole output until the size is known. When using the {@link FramedSnappyCompressorOutputStream} this limitation is taken care of by\n * the warpping framing format.\n * </p>\n *\n * @see <a href=""https://github.com/google/snappy/blob/master/format_description.txt"">Snappy compressed format description</a>\n * @since 1.14\n * @NotThreadSafe",public ,org.apache.commons.compress.compressors.CompressorOutputStream,
org.apache.commons.compress.compressors.pack200.Pack200Strategy,Enum,* Enumerates the different modes the Pack200 streams can use to wrap input and output.\n *\n * @since 1.3,public ,enum has no extends,
org.apache.commons.compress.compressors.pack200.AbstractStreamBridge,Abstract Class,* Provides an InputStream to read all data written to this OutputStream.\n *\n * @ThreadSafe\n * @since 1.3,abstract ,java.io.FilterOutputStream,
org.apache.commons.compress.compressors.pack200.InMemoryCachingStreamBridge,Class,* StreamBridge that caches all data written to the output side in memory.\n *\n * @since 1.3,final ,org.apache.commons.compress.compressors.pack200.AbstractStreamBridge,
org.apache.commons.compress.compressors.pack200.Pack200CompressorOutputStream,Class,* An output stream that compresses using the Pack200 format.\n *\n * @NotThreadSafe\n * @since 1.3,public ,org.apache.commons.compress.compressors.CompressorOutputStream,
org.apache.commons.compress.compressors.pack200.TempFileCachingStreamBridge,Class,* StreamBridge that caches all data written to the output side in a temporary file.\n *\n * @since 1.3,final ,org.apache.commons.compress.compressors.pack200.AbstractStreamBridge,
org.apache.commons.compress.compressors.pack200.Pack200CompressorInputStream,Class,* An input stream that decompresses from the Pack200 format to be read as any other stream.\n *\n * <p>\n * The {@link CompressorInputStream#getCount getCount} and {@link CompressorInputStream#getBytesRead getBytesRead} methods always return 0.\n * </p>\n *\n * @NotThreadSafe\n * @since 1.3,public ,org.apache.commons.compress.compressors.CompressorInputStream,
org.apache.commons.compress.compressors.pack200.Pack200Utils,Class,* Utility methods for Pack200.\n *\n * @ThreadSafe\n * @since 1.3,public ,,
org.apache.commons.compress.compressors.CompressorStreamProvider,Interface,* Creates Compressor {@link CompressorInputStream}s and {@link CompressorOutputStream}s.\n *\n * @since 1.13,public ,,
org.apache.commons.compress.compressors.deflate.DeflateCompressorInputStream,Class,* Deflate decompressor.\n *\n * @since 1.9,public ,org.apache.commons.compress.compressors.CompressorInputStream,org.apache.commons.compress.utils.InputStreamStatistics
org.apache.commons.compress.compressors.deflate.DeflateCompressorOutputStream,Class,* Deflate compressor.\n *\n * <em>Calling flush()</em>\n * <p>\n * Calling {@link #flush()} flushes the encoder and calls {@code outputStream.flush()}. All buffered pending data will then be decompressible from the output\n * stream. Calling this function very often may increase the compressed file size a lot.\n * </p>\n *\n * @since 1.9,public ,org.apache.commons.compress.compressors.CompressorOutputStream,
org.apache.commons.compress.compressors.deflate.DeflateParameters,Class,* Parameters for the Deflate compressor.\n *\n * @since 1.9,public ,,
org.apache.commons.compress.compressors.CompressorException,Class,* Signals that an Compressor exception of some sort has occurred.,public ,java.io.IOException,
org.apache.commons.compress.compressors.CompressorStreamFactory,Class,"* <p>\n * Creates a Compressor[In|Out]putStreams from names. To add other implementations you should extend CompressorStreamFactory and override the\n * appropriate methods (and call their implementation from super of course).\n * </p>\n *\n * Example (Compressing a file):\n *\n * <pre>\n * final OutputStream out = Files.newOutputStream(output.toPath());\n * CompressorOutputStream cos = new CompressorStreamFactory().createCompressorOutputStream(CompressorStreamFactory.BZIP2, out);\n * IOUtils.copy(Files.newInputStream(input.toPath()), cos);\n * cos.close();\n * </pre>\n *\n * Example (Decompressing a file):\n *\n * <pre>\n * final InputStream is = Files.newInputStream(input.toPath());\n * CompressorInputStream in = new CompressorStreamFactory().createCompressorInputStream(CompressorStreamFactory.BZIP2, is);\n * IOUtils.copy(in, Files.newOutputStream(output.toPath()));\n * in.close();\n * </pre>\n *\n * @Immutable provided that the deprecated method setDecompressConcatenated is not used.\n * @ThreadSafe even if the deprecated method setDecompressConcatenated is used",public ,,org.apache.commons.compress.compressors.CompressorStreamProvider
org.apache.commons.compress.compressors.zstandard.ZstdCompressorOutputStream,Class,"* {@link CompressorOutputStream} implementation to create Zstandard encoded stream. Library relies on <a href=""https://github.com/luben/zstd-jni/"">Zstandard\n * JNI</a>\n *\n * @since 1.16",public ,org.apache.commons.compress.compressors.CompressorOutputStream,
org.apache.commons.compress.compressors.zstandard.ZstdUtils,Class,* Utility code for the Zstandard compression format.\n *\n * @ThreadSafe\n * @since 1.16,public ,,
org.apache.commons.compress.compressors.zstandard.ZstdUtils$CachedAvailability,Enum,No Comment,,enum has no extends,
org.apache.commons.compress.compressors.zstandard.ZstdCompressorInputStream,Class,"* {@link CompressorInputStream} implementation to decode Zstandard encoded stream. Library relies on <a href=""https://github.com/luben/zstd-jni/"">Zstandard\n * JNI</a>\n *\n * @since 1.16",public ,org.apache.commons.compress.compressors.CompressorInputStream,org.apache.commons.compress.utils.InputStreamStatistics
org.apache.commons.compress.compressors.lz77support.AbstractLZ77CompressorInputStream,Abstract Class,"* Encapsulates code common to LZ77 decompressors.\n *\n * <p>\n * Assumes the stream consists of blocks of literal data and back-references (called copies) in any order. Of course the first block must be a literal block for\n * the scheme to work - unless the {@link #prefill prefill} method has been used to provide initial data that is never returned by {@link #read read} but only\n * used for back-references.\n * </p>\n *\n * <p>\n * Subclasses must override the three-arg {@link #read read} method as the no-arg version delegates to it and the default implementation delegates to the no-arg\n * version, leading to infinite mutual recursion and a {@code StackOverflowError} otherwise.\n * </p>\n *\n * <p>\n * The contract for subclasses' {@code read} implementation is:\n * </p>\n * <ul>\n *\n * <li>keep track of the current state of the stream. Is it inside a literal block or a back-reference or in-between blocks?</li>\n *\n * <li>Use {@link #readOneByte} to access the underlying stream directly.</li>\n *\n * <li>If a new literal block starts, use {@link #startLiteral} to tell this class about it and read the literal data using {@link #readLiteral} until it\n * returns {@code 0}. {@link #hasMoreDataInBlock} will return {@code false} before the next call to {@link #readLiteral} would return {@code 0}.</li>\n *\n * <li>If a new back-reference starts, use {@link #startBackReference} to tell this class about it and read the literal data using {@link #readBackReference}\n * until it returns {@code 0}. {@link #hasMoreDataInBlock} will return {@code false} before the next call to {@link #readBackReference} would return\n * {@code 0}.</li>\n *\n * <li>If the end of the stream has been reached, return {@code -1} as this class' methods will never do so themselves.</li>\n *\n * </ul>\n *\n * <p>\n * {@link #readOneByte} and {@link #readLiteral} update the counter for bytes read.\n * </p>\n *\n * @since 1.14","public , abstract ",org.apache.commons.compress.compressors.CompressorInputStream,org.apache.commons.compress.utils.InputStreamStatistics
org.apache.commons.compress.compressors.lz77support.Parameters,Class,* Parameters of the {@link LZ77Compressor compressor}.,"public , final ",,
org.apache.commons.compress.compressors.lz77support.Parameters$Builder,Class,* Builder for {@link Parameters} instances.,"public , static ",,
org.apache.commons.compress.compressors.lz77support.LZ77Compressor,Class,"* Helper class for compression algorithms that use the ideas of LZ77.\n *\n * <p>\n * Most LZ77 derived algorithms split input data into blocks of uncompressed data (called literal blocks) and back-references (pairs of offsets and lengths)\n * that state ""add {@code length} bytes that are the same as those already written starting {@code offset} bytes before the current position. The details of how\n * those blocks and back-references are encoded are quite different between the algorithms and some algorithms perform additional steps (Huffman encoding in the\n * case of DEFLATE for example).\n * </p>\n *\n * <p>\n * This class attempts to extract the core logic - finding back-references - so it can be re-used. It follows the algorithm explained in section 4 of RFC 1951\n * (DEFLATE) and currently doesn't implement the ""lazy match"" optimization. The three-byte hash function used in this class is the same as the one used by zlib\n * and InfoZIP's ZIP implementation of DEFLATE. The whole class is strongly inspired by InfoZIP's implementation.\n * </p>\n *\n * <p>\n * LZ77 is used vaguely here (as well as many other places that talk about it :-), LZSS would likely be closer to the truth but LZ77 has become the synonym for\n * a whole family of algorithms.\n * </p>\n *\n * <p>\n * The API consists of a compressor that is fed {@code byte}s and emits {@link Block}s to a registered callback where the blocks represent either\n * {@link LiteralBlock literal blocks}, {@link BackReference back-references} or {@link EOD end of data markers}. In order to ensure the callback receives all\n * information, the {@code #finish} method must be used once all data has been fed into the compressor.\n * </p>\n *\n * <p>\n * Several parameters influence the outcome of the ""compression"":\n * </p>\n * <dl>\n *\n * <dt>{@code windowSize}</dt>\n * <dd>the size of the sliding window, must be a power of two - this determines the maximum offset a back-reference can take. The compressor maintains a buffer\n * of twice of {@code windowSize} - real world values are in the area of 32k.</dd>\n *\n * <dt>{@code minBackReferenceLength}</dt>\n * <dd>Minimal length of a back-reference found. A true minimum of 3 is hard-coded inside of this implementation but bigger lengths can be configured.</dd>\n *\n * <dt>{@code maxBackReferenceLength}</dt>\n * <dd>Maximal length of a back-reference found.</dd>\n *\n * <dt>{@code maxOffset}</dt>\n * <dd>Maximal offset of a back-reference.</dd>\n *\n * <dt>{@code maxLiteralLength}</dt>\n * <dd>Maximal length of a literal block.</dd>\n * </dl>\n *\n * @see ""https://tools.ietf.org/html/rfc1951#section-4""\n * @since 1.14\n * @NotThreadSafe",public ,,
org.apache.commons.compress.compressors.lz77support.LZ77Compressor$BackReference,Class,* Represents a back-reference.,"public , static , final ",org.apache.commons.compress.compressors.lz77support.LZ77Compressor.AbstractReference,
org.apache.commons.compress.compressors.lz77support.LZ77Compressor$Block$BlockType,Enum,* Enumerates the block types the compressor emits.,public ,enum has no extends,
org.apache.commons.compress.compressors.lz77support.LZ77Compressor$Callback,Interface,* Callback invoked while the compressor processes data.\n     *\n     * <p>\n     * The callback is invoked on the same thread that receives the bytes to compress and may be invoked multiple times during the execution of\n     * {@link #compress} or {@link #finish}.\n     * </p>,public ,,
org.apache.commons.compress.compressors.lz77support.LZ77Compressor$EOD,Class,"A simple ""we are done"" marker.","public , static , final ",org.apache.commons.compress.compressors.lz77support.LZ77Compressor.Block,
org.apache.commons.compress.compressors.lz77support.LZ77Compressor$LiteralBlock,Class,"* Represents a literal block of data.\n     *\n     * <p>\n     * For performance reasons this encapsulates the real data, not a copy of it. Don't modify the data and process it inside of {@link Callback#accept}\n     * immediately as it will get overwritten sooner or later.\n     * </p>","public , static , final ",org.apache.commons.compress.compressors.lz77support.LZ77Compressor.AbstractReference,
org.apache.commons.compress.compressors.lz77support.LZ77Compressor$AbstractReference,Abstract Class,* Represents a back-reference.,"public , abstract , static ",org.apache.commons.compress.compressors.lz77support.LZ77Compressor.Block,
org.apache.commons.compress.compressors.lz77support.LZ77Compressor$Block,Abstract Class,* Base class representing blocks the compressor may emit.\n     *\n     * <p>\n     * This class is not supposed to be subclassed by classes outside of Commons Compress so it is considered internal and changed that would break subclasses\n     * may get introduced with future releases.\n     * </p>,"public , abstract , static ",,
org.apache.commons.compress.compressors.lz77support.LZ77Compressor$Block.BlockType,Enum,* Enumerates the block types the compressor emits.,public ,enum has no extends,
org.apache.commons.compress.compressors.lzma.LZMAUtils,Class,* Utility code for the LZMA compression format.\n *\n * @ThreadSafe\n * @since 1.10,public ,,
org.apache.commons.compress.compressors.lzma.LZMAUtils$CachedAvailability,Enum,No Comment,,enum has no extends,
org.apache.commons.compress.compressors.lzma.LZMACompressorOutputStream,Class,* LZMA compressor.\n *\n * @since 1.13,public ,org.apache.commons.compress.compressors.CompressorOutputStream,
org.apache.commons.compress.compressors.lzma.LZMACompressorInputStream,Class,* LZMA decompressor.\n *\n * @since 1.6,public ,org.apache.commons.compress.compressors.CompressorInputStream,org.apache.commons.compress.utils.InputStreamStatistics
org.apache.commons.compress.compressors.FileNameUtil,Class,* File name mapping code for the compression formats.\n *\n * @ThreadSafe\n * @since 1.4,public ,,
org.apache.commons.compress.compressors.bzip2.BZip2CompressorOutputStream,Class,"* An output stream that compresses into the BZip2 format into another stream.\n *\n * <p>\n * The compression requires large amounts of memory. Thus you should call the {@link #close() close()} method as soon as possible, to force\n * {@code BZip2CompressorOutputStream} to release the allocated memory.\n * </p>\n *\n * <p>\n * You can shrink the amount of allocated memory and maybe raise the compression speed by choosing a lower blocksize, which in turn may cause a lower\n * compression ratio. You can avoid unnecessary memory allocation by avoiding using a blocksize which is bigger than the size of the input.\n * </p>\n *\n * <p>\n * You can compute the memory usage for compressing by the following formula:\n * </p>\n *\n * <pre>\n * &lt;code&gt;400k + (9 * blocksize)&lt;/code&gt;.\n * </pre>\n *\n * <p>\n * To get the memory required for decompression by {@link BZip2CompressorInputStream} use\n * </p>\n *\n * <pre>\n * &lt;code&gt;65k + (5 * blocksize)&lt;/code&gt;.\n * </pre>\n *\n * <table style=""width:100%"" border=""1"">\n * <caption>Memory usage by blocksize</caption>\n * <tr>\n * <th colspan=""3"">Memory usage by blocksize</th>\n * </tr>\n * <tr>\n * <th style=""text-align: right"">Blocksize</th>\n * <th style=""text-align: right"">Compression<br>\n * memory usage</th>\n * <th style=""text-align: right"">Decompression<br>\n * memory usage</th>\n * </tr>\n * <tr>\n * <td style=""text-align: right"">100k</td>\n * <td style=""text-align: right"">1300k</td>\n * <td style=""text-align: right"">565k</td>\n * </tr>\n * <tr>\n * <td style=""text-align: right"">200k</td>\n * <td style=""text-align: right"">2200k</td>\n * <td style=""text-align: right"">1065k</td>\n * </tr>\n * <tr>\n * <td style=""text-align: right"">300k</td>\n * <td style=""text-align: right"">3100k</td>\n * <td style=""text-align: right"">1565k</td>\n * </tr>\n * <tr>\n * <td style=""text-align: right"">400k</td>\n * <td style=""text-align: right"">4000k</td>\n * <td style=""text-align: right"">2065k</td>\n * </tr>\n * <tr>\n * <td style=""text-align: right"">500k</td>\n * <td style=""text-align: right"">4900k</td>\n * <td style=""text-align: right"">2565k</td>\n * </tr>\n * <tr>\n * <td style=""text-align: right"">600k</td>\n * <td style=""text-align: right"">5800k</td>\n * <td style=""text-align: right"">3065k</td>\n * </tr>\n * <tr>\n * <td style=""text-align: right"">700k</td>\n * <td style=""text-align: right"">6700k</td>\n * <td style=""text-align: right"">3565k</td>\n * </tr>\n * <tr>\n * <td style=""text-align: right"">800k</td>\n * <td style=""text-align: right"">7600k</td>\n * <td style=""text-align: right"">4065k</td>\n * </tr>\n * <tr>\n * <td style=""text-align: right"">900k</td>\n * <td style=""text-align: right"">8500k</td>\n * <td style=""text-align: right"">4565k</td>\n * </tr>\n * </table>\n *\n * <p>\n * For decompression {@code BZip2CompressorInputStream} allocates less memory if the bzipped input is smaller than one block.\n * </p>\n *\n * <p>\n * Instances of this class are not threadsafe.\n * </p>\n *\n * <p>\n * TODO: Update to BZip2 1.0.1\n * </p>\n *\n * @NotThreadSafe",public ,org.apache.commons.compress.compressors.CompressorOutputStream,org.apache.commons.compress.compressors.bzip2.BZip2Constants
org.apache.commons.compress.compressors.bzip2.BZip2CompressorOutputStream$Data,Class,No Comment,"static , final ",,
org.apache.commons.compress.compressors.bzip2.BlockSort,Class,"* Encapsulates the Burrows-Wheeler sorting algorithm needed by {@link BZip2CompressorOutputStream}.\n *\n * <p>\n * This class is based on a Java port of Julian Seward's blocksort.c in his libbzip2\n * </p>\n *\n * <p>\n * The Burrows-Wheeler transform is a reversible transform of the original data that is supposed to group similar bytes close to each other. The idea is to sort\n * all permutations of the input and only keep the last byte of each permutation. E.g. for ""Commons Compress"" you'd get:\n * </p>\n *\n * <pre>\n *  CompressCommons\n * Commons Compress\n * CompressCommons\n * essCommons Compr\n * mmons CompressCo\n * mons CompressCom\n * mpressCommons Co\n * ns CompressCommo\n * ommons CompressC\n * ompressCommons C\n * ons CompressComm\n * pressCommons Com\n * ressCommons Comp\n * s CompressCommon\n * sCommons Compres\n * ssCommons Compre\n * </pre>\n *\n * <p>\n * Which results in a new text ""ss romooCCmmpnse"", in adition the index of the first line that contained the original text is kept - in this case it is 1. The\n * idea is that in a long English text all permutations that start with ""he"" are likely suffixes of a ""the"" and thus they end in ""t"" leading to a larger block\n * of ""t""s that can better be compressed by the subsequent Move-to-Front, run-length und Huffman encoding steps.\n * </p>\n *\n * <p>\n * For more information see for example:\n * </p>\n * <ul>\n * <li><a href=""http://www.hpl.hp.com/techreports/Compaq-DEC/SRC-RR-124.pdf"">Burrows, M. and Wheeler, D.: A Block-sorting Lossless Data Compression\n * Algorithm</a></li>\n * <li><a href=""http://webglimpse.net/pubs/suffix.pdf"">Manber, U. and Myers, G.: Suffix arrays: A new method for on-line string searches</a></li>\n * <li><a href=""http://www.cs.tufts.edu/~nr/comp150fp/archive/bob-sedgewick/fast-strings.pdf"">Bentley, J.L. and Sedgewick, R.: Fast Algorithms for Sorting and\n * Searching Strings</a></li>\n * </ul>\n *\n * @NotThreadSafe",final ,,
org.apache.commons.compress.compressors.bzip2.BZip2CompressorInputStream,Class,* An input stream that decompresses from the BZip2 format to be read as any other stream.\n *\n * @NotThreadSafe,public ,org.apache.commons.compress.compressors.CompressorInputStream,org.apache.commons.compress.compressors.bzip2.BZip2Constants;org.apache.commons.compress.utils.InputStreamStatistics
org.apache.commons.compress.compressors.bzip2.BZip2CompressorInputStream$Data,Class,No Comment,"private , static , final ",,
org.apache.commons.compress.compressors.bzip2.BZip2Utils,Abstract Class,* Utility code for the BZip2 compression format.\n *\n * @ThreadSafe\n * @since 1.1,"public , abstract ",,
org.apache.commons.compress.compressors.bzip2.CRC,Class,* A simple class the hold and calculate the cyclic redundancy check (CRC) for sanity checking of the data.\n *\n * @NotThreadSafe,final ,,
org.apache.commons.compress.compressors.bzip2.BZip2Constants,Interface,* Constants for both the compress and decompress BZip2 classes.,default,,
org.apache.commons.compress.compressors.bzip2.Rand,Class,* Random numbers for both the compress and decompress BZip2 classes.,final ,,
org.apache.commons.compress.MemoryLimitException,Class,"* If a stream checks for estimated memory allocation, and the estimate goes above the memory limit, this is thrown. This can also be thrown if a stream tries\n * to allocate a byte array that is larger than the allowable limit.\n *\n * @since 1.14",public ,java.io.IOException,
org.apache.commons.compress.PasswordRequiredException,Class,* Exception thrown when trying to read an encrypted entry or file without configuring a password.\n *\n * @since 1.10,public ,java.io.IOException,
